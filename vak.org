* Meetings
** <2025-02-07 Fri> overleg VAF
- word boundaries te bespreken
- common crawl : filter op periode is voor de crawling tijdens deze periode.  de informatie die hiermee wordt gevonden kan al veel ouder zijn.   enerzijds is dat een probleem.  anderszijds willen we ons ook niet beperken tot enkel nieuwe informatie tijdens deze periode gepubliceerd.  enkele nieuwe info is allicht relevanter voor social media.
- evaluatie criteratie relevant
- evaluatie criteria disinformatie
- bekijken welke data nu juist mee te nemen voor disinformatie?   lijst van URLs?
- feedback VAF : research questions beperken en groeperen.  andere zaken?
- TikTok API : test gedaan, technische verbinding is OK.
- EuroHPC : reminder gestuurd, spam folder student.ou.nl account?
 => Servicedesk, ITF <servicedesk@ou.nl>

AF :
introductie meer uitgebreid
related literature ook meer uitgebreid
ofwel per onderzoeksvraag 1 hoofdstuk
ofwel datapreparatie 1 hoofdstuk, dan model, ...

scenarios 

beslissingen in AF opsommen

https://guides.lib.uni.edu/media-accuracy-media-bia-media-trends/logical-fallacies

filtering, evaluation, ..
perhaps limit to one specific source of migrants.

rumour detection 1 class classification te bekijken?


* links

[[https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/][BM25 search algorithm]]
https://github.com/Florents-Tselai/WarcDB
https://github.com/iipc/jwarc
https://medium.com/@samuel.schaffhauser/using-the-common-crawl-as-a-data-source-693a41b3baa9
index lijst downloaden op https://medium.com/@samuel.schaffhauser/using-the-common-crawl-as-a-data-source-693a41b3baa9
https://pullpush.io
https://developers.tiktok.com/products/research-api/
https://sf16-va.tiktokcdn.com/obj/eden-va2/lapz_k4_rvarpa/ljhwZthlaukjlkulzlp/form/research-endorsement-letter.pdf
https://vast.ai/
https://www.shepbryan.com/blog/what-is-gguf

let op : er is EU disinfo lab en eu vs disinfo

https://www.disinfo.eu/publications/disinformation-landscape-in-the-netherlands/
https://www.disinfo.eu/publications/disinformation-landscape-in-belgium/

project dat in belgie socials checkte op disinformatie
https://crossover.social/

mee te nemen?
https://www.reddit.com/r/Antwerpen/


https://euvsdisinfo.eu/ukraine/

commonly used narratives :
https://edmo.eu/publications/disinformers-use-similar-arguments-and-techniques-to-steer-hate-against-migrants-from-ukraine-or-the-global-south-2/
https://benedmo.eu/
https://belux.edmo.eu
https://www.logicallyfacts.com/
https://www.migrationpolicy.org/article/disinformation-migration-how-fake-news-spreads
https://www.europarl.europa.eu/RegData/etudes/IDAN/2021/653641/EXPO_IDA(2021)653641_EN.pdf


https://crisiscentrum.be/nl/risicos-belgie/veiligheidsrisicos/desinformatie/desinformatie
->
https://www.mediawijs.be/nl
https://www.mediawijsheid.nl/nepnieuws/
https://www.watwat.be/fake-news/hoe-weet-ik-online-tekst-fotos-videos-echt-fake-zijn
https://www.isdatechtzo.nl/


EuroHPC support :
We can also be contacted by email at: servicedesk@lxp.lu
https://docs.lxp.lu/first-steps/handling_jobs/#viewing-jobs-in-the-queue

python scripts en modellen downloaden op voorhand :
https://docs.lxp.lu/howto/HFInference/

** tiktok
[[https://developers.tiktok.com/doc/vce-getting-started][dev account getting started]]
[[https://developers.tiktok.com/research/7467661064878098438][credentials dev account]]
[[https://developers.tiktok.com/doc/research-api-get-started][research api getting started]]
[[https://developers.tiktok.com/doc/client-access-token-management][generate an access token]]
[[https://developers.tiktok.com/doc/research-api-specs-query-videos?enter_method=left_navigation][API reference - query videos]]
[[https://developers.tiktok.com/doc/research-api-specs-query-video-comments?enter_method=left_navigation][API reference - query video comments]]

* datasets

[[https://www.kaggle.com/code/akshayr009/fakenewsdetection]]
https://www.kaggle.com/datasets/corrieaar/disinformation-articles
https://www.kaggle.com/datasets/imuhammad/euvsdisinfo-disinformation-database


* Mails

Dear Clara and Stefano,

I was working with the tiktok research API and noticed this is a severely limited API.  

Some limitations :

- max 1000 calls per day, max 100.000 results per day
- even if you ask for the maximum of 100 videos in a call, you almost always get less than that (due to deleted videos, ...), meaning the amount of videos you can retrieve will be substantially less than 100.000
- you need to throttle your calls, one call every 10 seconds works, but not more than that.
- resuming the download of a specific day/query seems difficult as results are returned in descending video_id, but you are not allowed to specify that you want all videos with a video_id < previous_last_video_id
- no decent filtering in api on keywords is available, so you need to download everything and then filter
- you cannot use extented time periods and need to fetch this in pieces.
- you do not know how many results a query will return in total

Since I don't know how many videos are created per day and I can only do 1000 calls per day, this poses a problem.

I was wondering whether you have used this API yourself?  If you have, in particular I would be interested in knowing if you succeeded in resuming from a last succesful id?

There is also the VCE environment that should allow for batch submissions which I will look into.

In the meantime, looking at the fields we can use to filter (and thus restrict the number of possible videos), I think the region_code might be interesting.  From the docs :

"A two digit code for the country where the video creator registered their account"

Could we limit the region_code to Belgium + the Netherlands + Ukraine + ... ?  If you see other countries that we should put into this list, please let me know.
perhaps we can play with the video_length field as well.   From the docs :

"The duration of the video SHORT: <15s MID: 15 ~60s LONG: 1~5min EXTRA_LONG: >5min"

A priori I would rather keep all videos, also the short ones, but if needed, this could be used as a filter as well.

** <2025-02-12 Wed> 

Dear Clara and Stefano,

I have tested the VCE (batch environment) of TikTok Research API.  Unfortunately, this environment only allows to fetch aggregated data, i.e. you can ask for counts of videos but not for individual video ids.  Other aggregates like max/min date etc are allowed but this is not what we are searching for.  Furthermore, filtering on countries is not supported.

That leaves us with the online API. To restate the main limitations : 1.000 API calls per day / max 100.000 items per day.  Many API calls return only a fraction of the max 100 items permitted per call.

It took me 3 days worth of API calls to download just the video ids (not even the comments yet) for a single day (2022-02-01), filtering on countries NL and BE.   Given that we are talking about a time period of almost 2 years (2022-02-01 -> 2024-11-30) and a couple of months to actually do the work, this is not going to work.  Also, most of the text is going to be in the comments, because a) most descriptions of videos are either absent or just a list of hashtags and b) the voice2text content was only present in 5 out of 71.460 videos.

We need a more agressive filtering strategy.  We could do this by :

- limiting time period
- limiting countries to just one (NL or BE)
- limiting to videos containing (many) comments- ... ?


Limiting to one country does not seem like a good strategy.   Even if we assume this would cut the number of videos in half, that would still mean 1 + 1/2 day of downloading per day just for the videos.
We could consider limiting the time period to the start of the war (first couple of weeks / months), where probably the most buzz around the war and its consequences was generated.
Limiting to videos containing many comments might be interesting.  p10 percentiles tell me p50 = 1, p80 = 5 and p90 = 12.  Please note if we focus on large comment counts, these comments have to be downloaded and this will "cost" API calls as well.

We could go for a combination of the above or come up with other strategies.

The research API does not allow to download videos, but there are tools that are capable of downloading videos given an id. If this works out, we could perhaps try some automatic audio transcription.

Interested in hearing your thoughts on this.

