* <2024-10-21 Mon> overleg Clara Maathuis 

te vragen :
- inschrijven op VAF : nog iets voor te doen?  "subject sheet" moet blijkbaar ingevuld worden en naar "study advisor" gestuurd worden.
- jeanine and Arjen
- overleaf template
- data wars ukrain, environment, immigration
- immigration is OK
- LLM safety check is dan wel een issue.  misschien moet er gejailbreaked worden
- dutch kan vertaald worden naar engels om engelstalige modellen te gebruiken
- explanation, reason a bit.
- llm based fact knowlegde based + entailment
- data : focus on text data, or transcripts of videos
- what is the workflow to get to a decision
- TODO lijstje van channels zoeken en doorsturen
- TODO beschikbaarheden doorsturen
- TODO send registration form (brightspace) 
- title : Disinformation detection using generative AI. => send mail to Jeanine Voncken , Clara 1, Stefano 2

| company              | newspaper                 | remarks                               |
|----------------------+---------------------------+---------------------------------------|
| VRT                  | online news               | national news company, more left wing |
| DPG media            | De Morgen                 | more left wing                        |
| DPG media            | Het laatste nieuws        | populist, more right wing             |
| Roularta Media Group | De Tijd                   | liberal, right wing                   |
| Roularta Media Group | Krant van West Vlaanderen | center                                |
| Roularta Media Group | De Zondag                 | center                                |
| Mediahuis            | Het nieuwsblad            | center                                |
| Mediahuis            | De Standaard              | center - right wing                   |
| Mediahuis            | Gazet van Antwerpen       | center                                |
| Mediahuis            | Het Belang van Limburg    | center                                |


* <2024-11-15 Fri> overleg VAF

** Alexander

- datasets : common crawl?   eventueel hate speech datasets (huggingface)?
- ingeschreven voor de VAF meetings (nog geen antwoord op gekrgen)
- er zijn nog geen vervolgmeetings ingepland

** Clara

structuur VAF
- intro 1 - 2 pages
- problem analysis / research background 2 - 3
- related research 15 - 20 papers.  at the end formulate knowledge gap. 3 - 4 pages
- research methodology: aim , RQs, 1 main question, 3 - 4 subquestions.  what do you expect to solve.   2 - 3 pages.  kiezen welke methodology
- risk analysis, in technical terms, research, data, CPU constraints, ...  control measures to reduce the risks


20 - 30 references in total

1 - 2 review meetings
end of VAF : presentation en zeker ook bij AF.  meestal bij studiedagen.


detection / classifiaton.  indien reasoning dan ook iets rond doen.  eventueel ook iets over hoe de argumenten worden opgebouwd.  eventueel rapporteren of we de gebruikte technieken 

tegen 6/12
related work
datasets

* <2024-11-20 Wed> begeleiding VAF vanuit OU 

tussentijdse presentatie is niet verplicht maar wel aangeraden.  best op een studiedag.
7 december den bosch

volgende bijeenkomsten vooral overleg met medestudenten, tips delen etc.

bij risico analyse zeker datasets vermelden.  + ook mitigeren, bv zelf dataset maken, of onderzoek bijstellen.

artikelen clusteren.


* <2024-12-06 Fri> overleg VAF

te stellen vragen :
- who is the chairman (to mention on front cover of the template)
- my related research only briefly mentions non LLM / transformer techniques.  should I focus more?
- what are we focusing on for the dataset(s) : 
  - channels : social media?  political parties and their stand on migration?   news articles ?
  - modus : should I make a dataset myself, label, ... or should I look for an existing dataset?  combination of both?
- methodology.  I'm not clear on what do actually do yet.  Do we want to include knowledge in the scope?  RAG?  sentiment analysis? reasoning?  can we do something using wikipedia?   I think the following articles have the most interesting setups : "The perils and promises of fact-checking with large language models", "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection", "On the Risk of Misinformation Pollution with Large Language Models"
- choice of LLM
  - almost all studies use GPT (openai).  we could also try to repoduce some study / approach using another LLM, or we could try GPT on another dataset (migration)

overleg :

NOS en RTL4 zijn meer open.  BENE datasets, news articles and political parties.
reddit api, tiktok

feb 2022 - november 2024 als timeline nemen om de oorlog mee te nemen.  discussies about immigrations raised a lot + verkiezingen.

eu disinfo mee te nemen.


include https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/ as one of the models.
 

for the dataset, we keep two options open :

1. preferred option.   we manually create a "BeNe immigration" dataset containing news about migrants, immigrations, refugees, asylum seekers, diasporas, ... mined for the period of February 2022 - November 2024 (justification : disinfo boomed because of Russia-Ukrain war, and also because of elections).  We will first consider news from publicly available news outlets from both Belgium (VRT) and the Netherlands (NOS, rtl4) (to be extracted using keywords in common crawl dumps), from political parties left to right (extract using keywords as well) and general websites.   If we don't have enough info, we can include social media accounts if possible (reddit developer API, twitter if possible, tiktok if possible).

2. if for whatever reason I cannot create a dataset myself (not enough info, no access, quality not good enough) we could consider existing fake news, disinformation or hate speech datasets as a backup.   But that would make the knowlegd gap smaller (societal impact of disinformation about immigration).   If the hate speech datasets are recent enough, chances are that part of the hate speech will be about immigrants anyways.

for the methodology :

- we focus on a content based approach (we ignore context)
- the goal is to do classification : is the news true or false
- base case is asking an LLM to classify news with some variation of prompts
- then introduce RAG setup.  we inject objective facts from knowledge sources (wikipedia, eudisinfo, others) and test which (combination of) sources helps classification.
- then refine RAG setup with sentiment analysis.  first do sentiment analysis using an existing model and inject news + external sources of objective facts + sentiment in the prompt of the LLM


research questions :
- RQ 1 : what kind of prompting would allow on a basic level to identify disinformation about immigration (zero-shot, few-shot, CoT)
- RQ 2 : can we identify more disinformation when introducting a RAG setup?
  subquestion : 
  - RQ 2.1 what are the sources to consult to help classify disinformation about immigrants (wikipedia facts, statistics, eudisinfo, ...)
- RQ 3 : can we identify more disinforation when introducing sentiment analysis in the RAG setup?


general remarks :
- we focus mainly on disinformation detection, not on fact checking
- dataset could be a deliverable for the thesis
- try the data collection process asap (don't wait for the end of VAF)

keywords : expats, migrants, migration, diasporas, asylum seekers + dutch (and french, if we wish to include the french speaking part of Belgium) translations of these terms.

actions on my part :
- write RQs in VAF proposal + desribe in one paragraph how to research this exactly
- look into reddit developer API
- check what datasets exist on eudisinfo platform.
- do some initial tests on data extraction using keywords
- do some initial tests on interrogating LLMs


I would appreciate input on :
- possible keywords to filter on.  both for news gathering and knowledge filtering in wikipedia dump.
- channels to mine (political parties, websites, news outlets, social media)
- what LLMs to test.   we discussed LlamaGuard.  should we evaluate multiple LLM models?  stick to one LLM?


* <2024-12-20 Fri> overleg VAF

te vragen :
- we have not discussed evaluation yet.  how will we evaluate the output of the LLMs?  my assumption is : dataset is created.  manual labelling (disinformation yes/no).   evaluate performance using classical metrics (accuracy, precision, recall, F1 depending on imbalance of dataset)
- about annotation : if it is manual labelling, how to judge whether disinformation or not.  where to find arguments on how to judge?  even if arguments are found, can we just take these and reuse them in the BeNe context?   there might be (geopolitical) differences.
- in section 4 methods : i'm not so sure what to put in subsection Research Method.   what should I put there that I haven't already put in the sections on the RQs?  I feel like I'm repeating myself in this section.
- regarding reddit.   I have found datasets on academictorrents.com.  I'm not sure this qualifies as a valid source of data?  is it known to you?   can it be used?


* <2024-12-20 Fri> overleg VAF

- try to open a support ticket in twitter, mention research
- try to open tiktok

RQ :
- graphrag te consideren? subquestion van RQ 2.  also add RAG and GRAPRAG in background

[OK] in planning : VAF stuk moet weg
[OK] in planning : split into RQ questions
[OK] AF part in planning : eerste zin wegdoen
[OK] include some holiday in planning
[OK] ook toevoegen report writing review (1 month)
[OK] ook toevoegen graduation + presentation

bij methodology : je moet verwijzen naar literatuur wanneer je metrics vermeldt
data science approach, uitleggen dataset split, hyperparameter tuning, ....

in 4 : research
[OK] eerst de aim opnemen als eerste zin

[OK] background chapter : algemene achtergrond van disinformatie, LLM, transformer.
[OK] in related research extra subchapter rond RAG op te nemen en GraphRAG.  daar zeggen welke metrieken er zijn gebruikt geweest.


* <2025-01-24 Fri> overleg VAF

- scraping van alle politieke partijen en news outlets van BeNe is gedaan.  daarna gefiltered op de keywords van Clara.  88422 hits.  news outlets zijn de grote slokoppen met 50+ % van de hits op 10 meest gevonden domains.  10% frans, 10% engels, rest nederlands.
- scraping van reddit subs gedaan, met filtering op keywords.  let wel, search is niet zo goed, dus displaced people hit ook op people.  2710 hits.
  subs: ["belgium", "Belgium2", "Belgium4", "belgie", "belgique", "nederlands", "Nederland", "Benelux", "Telegraaf", "Dutchnews", "PVV", "fvd", "PVDA_PTB", "VVD", "GroenLinks", "partijvoordedieren", "D66", "VoltNederland", "JA21", "Vooruit", "openvld", "VlaamsBelang"]
  keywords : ["asielzoeker", "asylum seeker", "buitenlander", "demandeur d'asile", "diaspora", "dispersed population", "displaced people", "emigrant", "expat", "expatriate", "expatrié", "fleeing population", "herplaatste bevolking", "immigrant", "immigratie", "immigration", "migratie", "migration", "ontheemde", "oorlogsvluchteling", "personne relocalisée", "personnes déplacées", "population dispersée", "population fuyante", "refugee", "relocated population", "réfugié", "réfugiés de guerre", "verspreide bevolking", "vluchteling", "vluchtende bevolking", "war refugees", "émigrant", "étranger", "migrant"]
- X support gecontacteerd maar geen enkele reactie
- tiktok ongoing.

* * <2025-01-24 Fri> overleg VAF 
- VAF report te finaliseren, gewacht tot deze meeting om zeker te zijn alle punten mee te hebben
- tiktok ongoing, kan nog enkele weken duren
- X meermaals support gecontacteerd, geen reactie
- reddit, news en political parties zijn gescraped.  zeer grote hoeveelheden hits.   stats gemaakt van aantallen hits + aantallen distinct hits.  strategie nodig!
- euroHPC toegang zou OK zijn
- volgende stappen AF?


na de grade inschrijven voor AF in brightspace
start with political parties.
title : semantics of hate (speech).

** <2025-02-07 Fri> overleg VAF
- word boundaries te bespreken
- common crawl : filter op periode is voor de crawling tijdens deze periode.  de informatie die hiermee wordt gevonden kan al veel ouder zijn.   enerzijds is dat een probleem.  anderszijds willen we ons ook niet beperken tot enkel nieuwe informatie tijdens deze periode gepubliceerd.  enkele nieuwe info is allicht relevanter voor social media.
- evaluatie criteratie relevant
- evaluatie criteria disinformatie
- bekijken welke data nu juist mee te nemen voor disinformatie?   lijst van URLs?
- feedback VAF : research questions beperken en groeperen.  andere zaken?
- TikTok API : test gedaan, technische verbinding is OK.
- EuroHPC : reminder gestuurd, spam folder student.ou.nl account?
 => Servicedesk, ITF <servicedesk@ou.nl>

AF :
introductie meer uitgebreid
related literature ook meer uitgebreid
ofwel per onderzoeksvraag 1 hoofdstuk
ofwel datapreparatie 1 hoofdstuk, dan model, ...

scenarios 

beslissingen in AF opsommen

https://guides.lib.uni.edu/media-accuracy-media-bia-media-trends/logical-fallacies

filtering, evaluation, ..
perhaps limit to one specific source of migrants.

rumour detection 1 class classification te bekijken?

* initial reading set

** (R) A comprehensive survey of multimodal fake news detection techniques: advances, challenges, and opportunities

studie van 2023

abstract :
er wordt gezegd dat naast methodes / modellen en datasets er ook uitgebreid wordt ingegaan op metrieken

introduction :
figure 4 geeft een mooi overzicht van unimodal text based aanpakken.  met feature extraction / selection en dan classificatie komen we tot een besluit fake ja / neen.
gebruik van GAN voor detectie van fake images
typische aanpak voor mixed modellen is dan elke modaliteit apart berekenen en dan combineren

architecture / life cycle :

origination -> propagation -> detection

met origination : for political gain, gaining revenue, defame
met propagation : social media, websites, blogs
met detection : fact checking (manual), fact checking (automatic), detection model

countermeasures : media literacy, fact-checking and regulations on social media.

current state : best combinatie automatic and manual fact checking
er zijn verschillende soorten fact checken, bv knowledge based, context based (checken van time and location of news), ...

propagation :
graph based neural models zijn gebruikt om de propagation patterns te onderzoeken (gebruikers labellen als ignorant, spreader, carrier, recovered)
vaak ook epidemic models (die gebruikt worden om de verspreiding van ziektes te modelleren) SIR model (susceptible - not yet exposed to misinfo, infectious - exposed and transmitting, removed- no longer exposed), SEIR model, SIS modeln, ... allerhande variaties
vaak gaan technieken uit van een statisch netwerk, maar in de praktijk komen er op sociale media vaak nodes en edges bij on the fly -> dynamic graph neural network.

! wel interessant om te vermelden in het stuk context based detection

detection :

fig 8 geeft overzicht van ML en DL modellen die zijn toegepast op fake news detection in social media.
klassieke ML modellen allerhande.  DL modellen.  maar ook ensemble modellen (combining the predictions of multiple models to improve overall accuracy and robustness)

er kan gebruik gemaakt worden van handmade feature extraction (bv aantal uitroeptekens, aantal emojis, ...).  CNNs kunnen zo'n features eventueel automatisch leren.
ensemble modellen : kunnen meerdere modellen zijn die samen enkel naar text kijken (bv CNN - LSTM) of een model voor text en een model voor video / image.
bedenking : dit soort modellen werkt allicht minder goed als LLMs gebruikt worden om fake news te genereren.

itt handmate features staan de deep learning modellen.  over het algemeen DL perf > ML perf met handmade features.
met DL approaches kan je ook multimodal werken, bv multimodal variational autoencoder (MVAE)

deepfakes / gans.

opsomming van veel gebruikte datasets in tables 4, 5 en 6.


attention based models :
naast gewone attention (puur op content), kan je ook 2stage attention doen : 1e stage is content based, 2e stage is user behaviour based
ook andere manieren van aanpak met eerst gewone text attention en dan er nog iets achter (bv CNN)
graph attention networks

metrieken :

ROC AUC, accuracy, precision, recall, F1.


challenges :
- multimodal : veelal is er een focus unimodal op text. nochtans kunnen andere modaliteiten cruciale info bevatten
- lack of explainability : zeker bij fake news moet je kunnen uitleggen waarom een beslissing is genomen (belangrijk voor trust)
- temporal dynamics : waarheid van een article kan veranderen in de tijd.  modellen hebben het hier moeilijk mee.
- manual fact checking : challenge omwille van througput, doorlooptijd, menselijke fouten en biases, etc
- multilingual datasets : veelal focus op engels
- not enough focus on auxiliary info : naast de content (text), moet er meer gekeken worden naar source / time of the news
- timely mitigation : fake news zo snel mogelijk detecteren / wegfilteren, maar kostelijk en niet alle aanpakken zijn er voor geschikt
  
** (R) A comprehensive survey on machine learning approaches for fake news detection

let wel : deze study is van eind 2022, begin 2023 gepubliceerd
er staan wel een aantal tabellen in die interessant kunnen zijn om belangrijke kenmerken van fake news op te sommen.

abstract :

3 manieren van aanpak : content based, context based, hybrid-based.

introduction :

fake news verspreidt sneller dan echt.  deels door de anonimiteit van de verzender.  het is ook extreem goedkoop om (fake) news te verspreiden via social media.
fake news wordt ook niet noodzakelijk gemaakt om anderen te overtuigen, maar kan ook uit pure financial gains gebeuren (clicks = ads = money).  dus 2 redenen : overtuigen en geld.
van alle fake news verspreidt political fake news zich het snelst.

preliminaries :

lange semantische discussie over de verschillende vormen van fake news.
bij traditional news media zijn er meerdere "checks-and-balance" vooraleer iets gepubliceerd wordt, waardoor er minder fake news is.
echo chamber effect : polarised groups that follow the belief system of the reader / participant.

existing FND approaches :

1. content based


afkortingen LR = logistic regression, naive bayes=NB, decision tree = DT, random forest = RF, gradient boost = GB, xgboost = XGB.

klassieke ML aanpak door features te extraheren en dan classificatie te doen met 1 van de vele mogelijkheden (SVM, K-NN, naieve bayes, etc).  features vaak stylistisch, bv ngram count, puncutations, ...
klassieke NLP aanpakken door term frequency (TF) en TF-IDF te gebruiken
features kunnen ook types van woorden zijn (PoS)
kijken naar negatieve emoties
kortom, stylistic and linguistic clues.

relatief weinig onderzoek met transformer based modellen.  de onderzoeken die wel zijn gedaan behalen allemaal betere resultaten dan klasssiekere ML technieken.  soms worden nog wat extra lagen achter een transformer based model gehangen.   geeft nog betere resultaten.


auteurs merken op dat multimodality zeer belangrijk is.   tweets met image in worden vaker geretweet.  niet duidelijk of dit in de hybrid based aanpakken nog aan bod komt?

   
2. context based


hier ook kijken naar user engagements en activities op de sociale netwerken.  het is wel moeilijk om deze data te scrapen.
vaak wordt er van uitgegaan dat fake news zich op een andere manier verspreidt dan echt nieuws.

een groot nadeel van deze aanpak is dat je niet onmiddellijk kan detecteren of iets fake news is.  je moet immers eerst de user interaction en propagation afwachten vooraleer je kan classifyen (en dan is het fake news al verspreid)

   
3. hybrid based


combineren van textuele, visuele en contexturele features.
je kan hiermee bijvoorbeeld eerst kijken naar content en daarna naar context.   beiden kunnen elkaar versterken.
kijken naar content en termporal context.

soms gebruiken van multimodale modellen, soms gebruik maken van 2 verschillende modellen : 1 model voor de tekst en 1 model voor de andere modus (image / video)


fake news characteristics

- longer texts, more capitalized words, fewer stop words, repitition, ...
- soorten van woorden die gebruikt worden zijn simpeler
- strong positive or negative sentiment of hate, anger, resentment
- focus on the present and the future


commonly used datasets :

LIAR, politicalnews, FEVER, buzzfeednews, some-like-it-hoax, zie Table 8.


methods

text classification
text moet eerst omgezet worden naar numerieke vectoren. typisch statistisch, maar dit neemt alleen frequencey features mee.  andere methodes kunnen ook semantiek capteren.   kan ofwel alleen frequency features van een woord meenemen, of ook meer context / semantische betekenis encoderen.  laatste tijd veel transformer based.

embeddings

statistische embeddings -> geen context
-> sparse, high dimensional vectors
- BoW: enkel het #occurrences van een woord telt als feature.   een document wordt enkel gekarakteriseerd door welke woorden hoe vaak voorkomen.  niet waar, niet in welke volorde.  alle woorden zijn even belangrijk. 
- TF-IDF: niet alle woorden zijn even belangrijk. herschalen van de frequency van woorden door vaak voorkomende te penaliseren.  tf-idf score van weinig voorkomend woord is hoog, vaak voorkomend is laag.  deze methode capteert ook geen semantiek.
  
context-independent pre-trained models : Word2Vec, GloVe -> wel semantic patterns maar geen context
words with similar meaning have similar representation -> dense, low dimensional vectors
- word2vec : king - man + woman = queen.  manieren om te leren : CBOW, skip-gram (predicten van volgende woorden door naar context te kijken)
- glove : performantere versie van word2vec

transformers -> ook context.
- BERT : bidirectional encoder representations from transformers.  deze capteren meer long range context, en dit van 2 kanten.  BERT is enkel de encoder kant van de transformer.  (GPT bv is unidirectional left-to-right)


ML algos

- CNNs in the context van NLP : feature extraction in "sliding window" van text, om er dan downstream iets mee te doen
- RNNs : gemaakt voor sequence input -> 1 output.  kan echter geen long term dependencies vinden (vanishing/exploding gradients)
- LSTM / GRUs : betere versie van RNN, wel longer term dependencies
- attention :
  - BERT : training met masked stukjes, vrij traag want veel params
  - DistilBert : optimized versie van bert
  - roberta : meer training data en dynamic masking gebruiken ipv fixed mask zoals bij bert.


current challenges :
- te simpele ML modellen : geen context, schaalt niet
- DL : beter maar nog geen long term dependencies
- transformer : current SoTA
- multimodaal : ook in images zit veel info, nog te capteren
- transparency : modellen vaak weinig transparant

** (R) A Survey on Multimodal Disinformation Detection

studie van september 2022

introduction :
de auteurs benadrukken dat bij disinformation zowel factualiteit als harmfullness even belangrijk zijn en tesamen moeten bekekeken worden.
multimodal is belangrijk, memes en videos verspreiden veel sneller en hebben veel meer impact.

figuur 1 zeer mooie figuur voor mis/dis/malinformatie

multimodal factuality checking :

- text : grootste deel van de research hier.
- images : komt sterk op op sociale media om disinformatie te verspreiden.  Fauxtography is defined as “visual images, especially news photographs, which convey a questionable (or outright false) sense of the events they seem to depict”
- audio / speech : gebruik van accoustisch signaal om de performantie te verbeteren.
- video : gebruikt voor deception detection.   facial expressions, eyebrow movements, ...
- network and temporal info : propagation and interaction networks van fake news zijn dieper en breder dan die van gewoon nieuws.  verspreidt sneller.


multimodal harmful content detection :

detecteren van de mate van harmfullness van news.
- text : detecteren van hate speech, offensive language, Qian et al 2018 : fine grained classification van hate speech, oa. ook anti-immigrant.
- image : cyberbullying, propaganda, memes,  altijd betere resultaten wanneer image + text samen bekeken worden
- speech / audio : detecting screaming / gunshots
- video : studies rond bv cyberbullying op youtube.  zeer moeilijk om te detecteren
- network and temporal info : harmful content vaak via coordinated actions (groups of users / bots)


modeling techniques :

verschillende manieren om multiple modalities te combineren :
- early fusion : low-level features van modaliteiten worden geleerd, gefused en dan in 1 model gestoken
- late fusion : unimodal decisions die daarna gefused wordn bv mbv voting mechanism
- hybrid fusion : een deel van de features gaan naar de final classifier, de rest wordt pas later gefused.


bijkomend zijn er unsupervised, semi-supervised, supervised en self-supervised aanpakken gebruikt
  
major challenges :

- hoe de multiple modalities combineren?
- datasets : moet meerdere modaliteiten bevatten, en naar factuality en harmfullness kijken
- explainability : uitleggen waarom iets geclassificeerd wordt als harmfull / niet juist
    
** (R) Analyzing the Role of Ukrainian and Russian Diaspora in Disinformation Campaigns

diaspora = a group of people who spread from one original country to other countries, or the act of spreading in this way.
reeds veel onderzoek naar disinformatie op social media.   hier specifiek gekeken naar de rol van diaspora in disinformation propagation.

RQ : What is the interaction between diaspora users and disinformation campaigns on Twitter surrounding the initial phase of the ongoing Russian-Ukraine conflict?

disinformation implies fabricating and using intentionally altered or false information while misinformation implies unintentionally building and using altered or false information.

** (R) Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection

kijken of LLMs fake news kunnen detecteren.   blijkt dat fined tuned BERT based SLM (small language model) nog beter is.   maar dat LLMs wel fake news kunnen detecteren maar vooral goede multi perspective rationales kunnen genereren.  deze rationales worden in een ARG (adaptive rationale guidance) network door een LLM gegenereerd en dan door een fine tuned SLM model gehaald.  ARG-D variant waarbij distillatie wordt gebruikt en er geen LLM prompting nodig is (cost effective)

LLMs zijn slecht in "veracity" te beoordelen maar zijn goed in content analyzeren.

de rationales laten genereren doe je door chain of thought prompting (let's take this step by step)
de LLMS genereren dan rationales en de SLMs gaan hiermee dan verder.  de combinatie helpt om betere resultaten te halen.

de auteurs ontwikkelen 2 varianten :
- ARG : versie waarbij een LLM rationales genereert en deze later gebruikt worden.   3 componenten 1/ encoding / rationales 2/ news rationale collaboration 3/ prediction
- ARG-D : kost effectieve versie waarbij geen LLMs gebruikt worden om rationales te genereren en enkel de componenten 1/ en 2/ overblijven (met 1/ vereenvoudigd zonder LLM)


De auteurs hinten hier op kennis in het ARG-D model dat geextraheerd wordt uit de ARG variant maar mij is na eerste lezing niet duidelijk wat hier juist mee bedoeld wordt.    het schijnt wel fundamenteel te zijn, want resultaten zijn ARG > ARG-D > al de rest.  dus zelfs ARG-D scoort al beter dan al de rest, en dit zonder het gebruik van (dure) LLMs.

nog een experiment gedaan waarbij een mix van ARG en ARG-D gebruikt wordt : we gebruiken a priori ARG-D, maar afhankelijk van de confidence daarvan wordt al dan niet ARG ondervraagd.   Door 23% van de data naar ARG te sturen, wordt een even goed resultaat behaald als door alles naar ARG te sturen (met een grote kostreductie tot gevolg).

algemeen :
fake news detection 2 generieke manier van aanpakken :
- social context baxed : kijken naar propagation patterns, user feedback en social network
- content based : kijken naar de inhoud van de tekst zelf

** (R) Can Large Language Models Detect Rumors on Social Media?

opstellen van een "LeRuD" model, waarbij prompts worden geengineerd om hints over rumors op te pikken in social media conversaties.

key om rumor detection te doen is om de propagation patterns (news + comments) te modelleren.  er wordt een Chain-of-propagation opgesteld.

hier zijn prompts opgesteld die zeggen dat :
- hier is een stuk nieuws, gegeven de writing style en de commonsense knowledge, is dit echt of niet ("rational" prompts)
- hier zijn comments op nieuws, zoek of er rebuttals (tegenspraak, mensen die het nieuws in twijfel trekken) of conflicts zijn en kijk of dit echt is of niet ("conflicting" prompts)


deze aanpak werkt maar in zekere mate door de context limiet van LLMs.  daarom worden de comments in blokken van k gegroepeerd en 1 voor 1 naar de LLM gestuurd.  de output van de laatste block wordt als output gebruikt

data leakage risk : dit gebeurt wanneer je een vraag stelt over iets waarop de LLM al getrained is.   maw dit zit in de kennis databank van de LLM.  de LLM kan dan onmiddellijk verifieren dat dit news inderdaad echt is

** (R) Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study

abstract :
men gebruikt LLMs om zowel naar de content als naar de propagation van misinformation te kijken.   LLMs schijnen goed te scoren op content maar minder goed te score op propagation.  daarna design 4 instruction tuned strategies om LLMs beter te trainen hierop.

hier wordt een empirical study gedaan om de performantie te vergelijken van een instruction-tuned LLM enerzijds en fine-tuned small models anderzijds for misinformation detection.

voornaamste conclusies :
- LLMS met vanilla prompts = ongeveer even goed als fine  tuned small models
- task description toevoegen in promts helpt voor LLMs.  CoT helpt niet
- meer training samples helpt niet consistent verbeteren
- propagation structure : LLMs zijn niet in staat om  propagation structure te begrijpen en scoren hier minder
  

2 specifieke instruction tuned strategies voor betere in-context resultaten :
- alternating sample learning: adjust position of samples to learn potential interactions from limited samples
- hard sample learning: learn more task relevant features from hard samples

  
2 specifieke instruction tuned strategies voor betere propagation resultaten :
- encourage LLMs to refine the propagation structure with self-prompting
- adapt propagation description languages to LLM


eerst algemene testen : 

de moeilijkheid bij content based misinformation detection is dat misinformation dikwijls in de stijl van "echt" nieuws wordt geschreven.

er worden een aantal experimenten gedaan  met verschillende prompts, N-shot learning, zowel voor content als voor propagation

interessante paper om na te denken over mogelijke prompting technieken.

N-shot learning werkt maar vanaf grotere N.  bij N=1 krijg je zelfs slechtere accuracy dan bij N=0.  bij N>1 gaat de accuracy omhoog (auteurs hebben getest tot 5)


voor propagation zijn een aantal testen gedaan, waarbij telkens eerst enkel de content wordt meegegeven, dan ook de comments en dan tenslotte ook de relaties (dus Content=1, Comments=0, Relaties=0, dan Content=1, Comments=1, Relaties=0 en dan Content=1, Comments=1, Relaties=1).  toevoegen comments helpt acc.  toevoegen relaties niet.  N-shot prompting helpt hier niet veel (enkele procenten)
  
  
dan instruction tuned testen :

alternating sample learning: afwisselend een positief en een negatief voorbeeld meegeven in de prompt.
hard sample learning: moeilijke voorbeelden gebruiken om sneller te leren.  moeilijk voorbeeld = als volgens 3 verschillende prompts de LLM consistent fout antwoordt tov de ground truth.

voornamelijk de hard sample learning lijkt de acc sterk te verbeteren


format graph input : expliciet opnemen van graph structuur en concepten van graphen in de prompt (node, edge, ...)
refining structure : eerst de (vaak complexe) propagation / interaction structuur doorsturen naar een LLM met de vraag om dit sterk te vereenvoudigen, en deze vereenvoudigde versie te gebruiken.

** (R) Challenges for Automatic Detection of Fake News Related to Migration : Invited paper

probleemstelling : fake news rond immigratie is vaak multi-lingual en multi-modal (text, image, video, audio)

interessante data bron : infomigrants.net die een aantal misinformaties debunken in meerdere talen.

paper onderzoekt een aantal "challenges" om op semi-automatische wijze fake informatie te debunken en de "recipients" (migranten) duidelijke en correct info te verschaffen.

er zijn heel wat platformen en tools (inclusief browser plugins) voor consumers en professionals (journalisten) om de waarheid van tekst te onderzoeken.  voor video tools om alterations of fake videos te detecteren.

NELA-GT and FakeNewsCorpus are datasets mined from a number of media websites, focusing mainly on US politics.

er zijn een aantal challenges rond het kunnen verifieren van data (datasets beschikbaarheid, schaal van de data, ...).   maar ook collaborative / explainable AI moet deel van de oplossing zijn

in deze paper worden geen oplossingen voorgesteld, enkel problemen opgelijst.  maar mss wel interessant om enkele datasets te bekijken.

** (R) Design Lessons from Building Deep Learning Disinformation Generation and Detection Solutions

in het artikel wordt een GAN architectuur voorgesteld om misinformatie niet alleen van de kant van de ontvanger te bekijken, maar ook van de kant van de zender.   een GAN genereert en detecteert misinformatie.   een aantal design considerations worden voorgesteld om AI systemen voor misinformatie op te stellen.

** (R) Detecting Propaganda in News Articles Using Large Language Models

in dit artikel wordt gefocused op het detecteren van gebruikte propaganda technieken via prompt refinement.  doel is om de LLM te laten genereren welke van (18) distinct propaganda techniques eventueel gebruikt zijn en om te classifyen of iets propaganda is of niet.  gebruikt openai gpt-3.5 turbo model.

propaganda techniques = methods used to manipulate public opinion.

de RQ is : how can gpt-3.5 model gebruikt worden om de verschillende types van propaganda techniques te detecteren in nieuwsartikelen?

belangrijke opmerking : een simpele classificatie ja/neen dit is fake news volstaat in feite niet zonder een explanation.   je moet de classificatie dan maar slikken.  en bij bv 70% accuracy betekent dat in 30% van de gevallen een foute classificatie.

aanpak :

begonnen met aan het model te vragen om zelf een prompt te schrijven ivm de propaganda techniques van Martino.
uiteindelijk gekomen op vorm : propaganda technieken -> instructions -> article content.

een aantal iteraties van de prompt worden beschreven, telkens met wat het model antwoordt en welke problemen daarmee zijn.

resultaten :
op gelabelde dataset slechts accuracy van 25%.  te verklaren door :
- LLM vindt altijd wel 1 of 2 mogelijke technieken
- LLM twijfelt wanneer er werkelijk veel technieken gebruikt zijn
- men heeft moeten filteren in de artikels op korte artikels om de token limiet van de openai api niet te overschrijden


sommige technieken blijken overmatig veel te worden voorspeld door de LLM.

voornaamste beperking is de token limiet van de openai api

interessant artikel om inspiratie op te doen naar gebruikte prompts.

** (R) Detecting and responding to hostile disinformation activities on social media using machine learning and deep neural networks

auteurs willen ai tools ontwikkelen voor near real time detection van disinformation campaigns.   automatische classificatie van social media posts als fake news of real news.

gebruik van "the dark crawler", tensorflow, random forest, libshorttext, liblinear en posit.

estimate : 5-9% van alle twitter accounts zijn bots en die zorgen voor 24% van alle tweets.

belangrijke disinformatie verspreiders zijn rusland, china, iran.   maar ook far righ domestic groups.

de auteurs sommen een aantal eerdere pogingen op rond detectie van fake news.

methodology :

the dark crawler om dark web fora te scrapen
preprocessing, v sentence detector, tokenizer, POS tagger, chunker via OpenNLP.  voornamelijk gebruikt om de tweets te verrijken met extra statistics (126 extra features worden toegevoegd).
met tensorflow een DNN gebouwd om te classifyen in real/fake/other.  randomiseren van de volgorde van de data om te voorkomen dat alle fake / real tweets dicht bij elkaar zouden zitten
libshorttext : lib om korte teksten (tweets) te transformeren in sparse feature vectors.
liblinear : classification program
random forest

totaal model is dan crawler -> OpenNLP -> 4 modellen maken
4 modellen zijn random forest, DNN, liblinear en libshorttext.  elk model genereert output, voor elk model kan gewicht ingesteld worden en er komt een finale predictie uit.

bijkomend gebruik gemaakt van posit (om pre-classificatie te doen??).  toolkit om allerhande statistics / features te genereren voor tekst.  hier ook gebruikt op character level, bv om te kijken hoe vaak uitroeptekens gebruikt worden.  WEKA om na te gaan hoe goed de (pre-)classificatie (??) is?

dataset : bijna 3 miljoen tweets, enkel engels overgehouden tot 2 miljoen

zeer uitgebreide uitleg over hoe datasets zijn gevonden, oa door te kijken naar welke facebook accounts ge-owned zijn door questionable sources (mediabiasfactcheck.com).
uiterst uitgebreide testing setup waarbij "double-blind" checks gebeuren tussen de verschillende modellen

vermelding van een study die expliciet disinformatie ivm immigratie heeft onderzocht : https://link.springer.com/article/10.1057/s42984-020-00029-4

hier wordt dus niets van LLMs gebruikt.  

** (R) Diasporas as Targets of Putin's Media: Categorizing the EU's Response to Kremlin-led Disinformation

artikel over master thesis rond disinformatie van rusland getarget naar russische diasporas in europa.   hypothese was dat er minder en minder effectieve reactie zou zijn van de EU in het geval van EU lidstaten in het "oosten" (latvia, ukraine) dan andere (duitsland).  dat bleek niet het geval te zijn.

master in european studies, dus niet echt AI focused.

DATM disinformation = Diasporas As Targets of Media.

vaak hebben diasporas nog een grote link met rusland en ontwikkelen ze eigen gewoontes rond russischtalige media, die gecontroleerd worden door rusland.  hierop wordt disinformatie verspreid.

voornamelijke in voormalige oostblok landen

main RQ : is there a variation in the EU's response to DATN disinformation depending on the country where such disinformation takes place? hypothese was dat er minder en minder effectieve reactie zou zijn van de EU


antwoorden van de EU kunnen zijn :

- verbal statement
- adoption of sanctions
- implementation of anti-disinformation policies


3 grote blokken :

- disinformation conceptual framing
- EU responses
- EU response categorization
  

disinformation conceptual framing

disinformatie vs propaganda : disinformatie vnl via fear and anger.  propaganda : more positive emotions to create sympathy

vanuit russisch perspectief :

vnl om uitbreiding van nato en US richting het oosten tegen te gaan.
voornaamste target zijn russische diaspora (al dan niet in ex sovject staten)

modus operandi :

- troll farms
- alluring tools : internet bubbels creeeren van like minded people
- allerhand audiovisuele media, tv zenders, ...


vanuit EU perspectief :

voornamelijk initiatieven gestart bij crisissen (annexatie Krim, brexit, covid, ...)


Latvia :

40% van de bevolking spreekt russisch (is diaspora) en is nauwelijks geintegreerd met de natives.  grote afhankelijkheid van russisch gas, low quality media en geen specifieke disinformation agencies.

97% van de diaspora kijkt naar russische media outlets ipv latvian outlets.

de boodschap die gestuurd wordt is vaak dat de diaspora slecht behandeld wordt.  anti-EU narrative (divided, inefficient, etc).  idem anti US/NATO narrative.

initiatieven van de staat : gevaar wordt erkend

EU response :

- ESCTF task force opgericht in 2015.  gericht op het tegengaan van disinformatie en "promoting EU values" in oostelijke landen.  deze task force heeft wel maar een beperkt(er) budget en FTEs dan de rusland counterpart.
- BCME : baltic center for media excellence (gericht op countering disinformatie)
- 2022 ban van RT en Sputnik (media outlets / tv kanalen).   dit kan relatief eenvoudig gebypassed worden door internet mirrors.  daarenboven zijn er nog meer russische media outlets, dus je hebt niet alles tegengehouden


conclusie van de auteur : Latvia ligt dicht bij Moskou en krijgt veel russische disinformatie te slikken.   de response van de EU is maar zo-zo.


Ukraine :

itt latvia zijn de russiche diaspora niet slecht geintegreerd.  taal barriere is klein.   makkelijk om te participeren in public life

veel media kanalen lijken wel onafhankelijk maar zijn het in feite niet.


boodschap naar diaspora is die van een gefaalde staat (ukraine), zogezegde onderdrukking.   nationalist = fascist verhaal.  soms ook het tegenover gestelde verhaal : ukraine en rusland zijn broederstaten.  er is geen verschil in identiteit.  verder anti EU/NATO/...

initiatieven van de staat : 

- bannen van tv kanalen
- bannen van journalisten
- wetten gestemd tegen russische media anawezigheid
- organisaties opgericht voor media literacy etc.

EU response :

- ESCTF task force opgericht in 2015.
- EP resolution voor strategische communicatie to counteract disinformatie tegen de EU (rechtstreeks gevolg van ukraine)
- center of excellence opgericht voor countering hybrid threats
- EU wide ban on russian media.  in de praktijk zijn de memberstates nog min of meer vrij om te volgen
  
conclusie van de auteur :
- EU doet hier duidelijk meer.  maar, het blijft toch allemaal redelijk vaag. daarenboven richt de EU zich op zijn member states, en niet op ukraine.

  
Germany : 

grootste russische diaspora in europa.  goed geintegreerd, brede toegang tot russische media.
zowel afd als "die linke" worden gesteund door russische media.


boodschap naar diaspora is  anti EU/NATO/Merkel.   verder inzetten op marginaliseren / criminaliseren van buitenlanders / immigranten om zo steun voor AFD te verkrijgen

initiatieven van de staat : 

- vooral inzetten op enhancen van media literacy skills (+ fact checking)
- network enforcement act
- deze initatieven zijn vrij beperkt, bv omdat ze graag een goede relatie met rusland willen, niet gedurende een lange periode reeds onder attack van rusland liggen

EU response :
- EUFactCheck (fact checking initiatief).   maar dit is beperkt aangzien ze enkel uitspraken van EU public figures natrekken.
- 2 plannen : EDAP en EMAAP.  expliciete plannen voor o.a. een database met media outlets.  ook deels gericht op "vrije verkiezingen".  MAAR : verbiedt enkel illegale content, en disinformation is niet noodzakelijk illegaal.
- bannen van RT deutsch tv kanaal

conclusie van de auteur :
- zowel EU als gemany doen vrij weinig.


algemene conclusies van de auteur

- EU vaak te laat, te weinig en overlappend met bestaande maatregelen (bv latvia)
- er is idd een verschil in response van de EU tussen verschillende landen, maar niet zoals in de main hypothese werd verondersteld.
  
** (R) Disinformation: analysis and identification

auteurs bouwen een classifier voor disinformatie, door linguistic clues rond stijl te bekijken, clickbaityness te evalueren en door veracity-based (veracity = waarheid) classificatie te doen .  voor de veracity based classifier werd een dataset aangemaakt.

disinformatie vaak mix van echte element en foute conclusies of leugens.  concreet zijn er classifiers gebouwd voor :
- onderscheiden van disinformatie articles van gewone articles
- onderscheiden van clickbait titles van andere titles
- fine-grained classification van disinformatie in veracity-based labels
  

automatische scale up van manuele fact checking is nodig.

1. onderscheiden van disinformatie van andere informatie :

er wordt gekeken naar hoax, propaganda, satire.

hier worden een aantal verschillende model architecturen opgebouwd :
- max-ent : baseline model TF-IDF (up to 3-grams)
- DNN met 2 dense layers.  300K vocab size
- bidirectionel LSTM (512 LSTM units)
  

klassieke training met adam optimizer en dan softmax clasification in 4 labels.  uit dit experiment blijkt dat DNN het beste scoort, maar dat disinformatie slechter classificeert dan satire.

2. onderscheiden disinformatie en clickbait titles

hier werd een Bi-LSTM model voor gemaakt.  zeer sterke performance (99%).  gek genoeg worden maar in 36% van disinformatie artikels clickbait titles gebruikt.

3. degree of veracity

vorige 2 aanpakken volstaan niet.  auteurs argumenteren dat automatic fact checking een noodzakelijk onderdeel vormt.  hierbij ga je een artikel een veractiy toekennen door factual statements te identifieren en die daana ook af te checken bij fact checking sources.

een nieuwe dataset gebaseerd op US elections 2016 werd aangemaakt.  manueel geannotteerd en 5 labels konden toegekend worden (false, partial truth, opinions stated as faccts, true, no evidence)

goede website om media bias te checken : https://mediabiasfactcheck.com/

ook hier een aantal modellen gemaakt DNN, Bi-LSTM (zowel word als character embeddings)

leren van een NLI task via BERT uncased model.  eerste N woorden van een artikel combineren met laatste N woorden van evidence om daarmee te classifyen.

4. automatische fact-checking web application (FactFinder)


sample entitities and actions from article -> gather evidence from the web -> use the evidence to classify into veracity class

dit is dus de NLI task hierboven beschreven maar dan geautomatiseerd.

belangrijkste conclusies :

- style based classification is niet genoeg om goede classificatie van clickbait / disinformatie te doen
- fact checking is nodig voor "credible debunking"  

** (R) Disinformation Detection: An Evolving Challenge in the Age of LLMs

gevaar : LLMS kunnen disinformatie genereren.

afkorting : LLMGD = LLM generated disinformatie.  paper focust expliciet hierop.

3 RQs :
- kunnen de huidige disinformatie detectie methoden LLMGD detecteren?
- indien niet, kunnen LLMs dan LLMGD detecteren?
- indien niet, kunnen we nieuwe aanpakken verzinnen om LLMGD te detecteren?

!!! er wordt verwezen naar een aantal andere modellen die voordien werden gebruikt om disinformatie detectie te doen, vnl SLM bv BERT, GPT-2 en T5.

ze starten met een dataset met human written news articles die categorie hebben gekregen fake/true.  op basis van deze dataset 3 nieuwe datasets genereren met chatgpt met 3 verschillende prompt techniques (standard prompt, mixture prompt, chain of thought prompt).


eerste conclusies :

- huidige (SLM) approaches zoals roberta fine tuned model kunnen nog wel heel simpele LLMGD ontdekken, maar falen volledig (bijna 80% misses) bij advanced LLMGD.
- vanilla GPT kan zijn eigen LLMGD niet onderscheiden
- door het gebruik van een "carefully crafted" chain of thought prompt kan de accuracy wel sterk verbeteren


datasets :

- human generated dataset is baseline.  men pakt de "Fake and Real news dataset" die bestaat uit ongeveer 50/50 echt nieuws van Reuters en van fake news uit bronnen allerhande.
- dan een dataset laten genereren door LLM met standard prompt (dwz minimaal de human written content aanpassen, door samen te vatten of formal tone te vragen)
- dan een dataset laten genereren door LLM met mixed prompt (dwz true news mixen met fake news)
- dan een dataset laten genereren door LLM met CoT prompt (dwz COT gebruiken om te tonen hoe het cognitieve proces van mensen bij het schrijven van disinformatie werkt.  who, what, how, where, when and why).  hier worden nog variaties in media outlets gegenereerd, dwz CNN (liberal), FOX (conservative) en Reuters (neutral)


experimenten

RQ1 :

roberta based fine tuned model dat oorspronkelijk getrained werd op human written news articles.  werkt zeer goed voor human written (nauwelijks fouten), werkt nog altijd zeer goed op LLMGD_standard, maar minder goed op LLMGD_mix (15% fout) en slecht op LLMGD_COT (78% fout).
verder stellen de auteurs vast dat er een political bias is : center leaning disinformation wordt vaker als true geclassifieerd.


RQ2 :
gevraagd aan GPT : does this news [...] contain any misleading information?  dit dan nog met of zonder bijkomende vraag naar verdere explanation.

results : gpt4 iets beter dan 3.5.  en vragen naar het analytical process voor ja/nee te antwoorden verbetert de accuracy gevoelig.  toch blijft de accuracy nog onder die van de roberta aanpak.  maw LLMs zonder meer niet geschikt voor detectie van disinformatie.


RQ3 :

vaststellingen :
- betere resultaten wanneer analytisch proces gevraagd wordt
- er moet kunnen ge-fact-checked worden


daarom hebben ze een specialised CoT prompt ontwikkeld.  dus naast de vraag extract all entitities, staan daar ook de vragen in "assess the factualness of the extracted events.  show your analytical process."  idem voor relaties tussen entiteiten.
hier zijn een aantal ablation studies op gebeurd, bv met/zonder persoonextractie, met/zonder place extractie, ..., geen explanation, schalen van 1 - 100.

kritische factoren blijken "event" en "time" elements te zijn.

** (R) Disinformation on refugees from Ukraine: Boosting Europe’s resilience after Russia’s invasion Alberto-Horst Neidhardt

paper focust op disinformatie rond ukrain refugees die het land ontvluchten omwille van de oorlog.  en de antwoorden op deze disinformatie.  welke actoren verspreiden en hoe?

auteur pusht voor een prebunking approach ipv debunking.  snellere interventies, meer anticipatie, meer skills ontwikkelen op voorhand.

groot deel van disinformatie gaat over refugees (brengen criminaliteit mee, krijgen voorkeursbehandeling, ...).  bedreiging voor europas health,wealth en identity.
voorlopig nog wel positieve sfeer tov ukraine, dus nog niet wijd verspreid.  wel solidarity fatigue.

hier wordt een beetje hetzelfde verhaal verteld als in 1 van de andere papers : er zijn wel initiatieven genomen in de EU, maar laat nog wat te wensen over.   vergeleken met bv ukraine waar ze wel al aan prebunking doen, moet de EU nog stappen nemen.

EU focust zich op foreign actors and kremlin-led disinformation campagins.  maar disinformatie kan evengoed van binnen de EU komen (bv right-wing)
daarenboven zou de EU zich meer moeten richten op segmenten van de bevolking die meer risico lopen om getarget te worden door disinformatie.

dus van reactive -> proactive (prebunking).  dit impliceert :
- explicit warning of an impending threat
- awareness of manipuliation techniques


who is spreading disinformation and on what channels?

- russian media outlets
- russian content on social media platforms like facebook or twitter.
- dit zijn dus externe bronnen van disinformatie (extern aan de EU)
- maar disinformatie wordt ook intern verspreid, of ook bv door gewoon nieuwe domein namen aan te maken en dezelfde content te verspreiden
- kan ook intern en anoniem zijn.   bv websites, fora, telegram, ...
- veelal security, violence, betere behandeling dan inheemse bevolking, of zelfs discriminatie van de bevolking tov buitenlanders,

why is migration disinformation so pervasive?

migratie leent zich tot disinformatie want :

- het is een complex fenomeen, moeilijk uit te leggen met feiten maar makkelijk met halve waarheden te verspreiden
- migratie heeft grote symbolische betekenis, raak aan religie / identiteit, jobs en security
- exploits the voiclessness van de subjects they target, gezien refugees weinig voice in de media hebben
- great replacement conspiracy theory

how do disinformation narratives change across space and time?

narratives komen altijd neer op health, wealth, identity maar wijzigen wel in de tijd, al naargelang de hot topics (bv covid19).  narratives worden gealigneerd met de waarden en angsten van bevolkingsgroepen.

specifiek voor ukraine speelt het identiteits verhaal niet zo hard, omdat ukrainse identiteit nogal overenkomt met de west europese.  hier wordt vooral op security gevoel gespeeld.

EU at crossroads?

voorlopig nog geen zeer grote tractie door disinformatie, maar aandeel disinformatie stijgt.
op een gegeven moment kan de publieke opinie kantelen in het nadeel van refugees.


Boosting resilience against disinformation

prebunking = detecteren en tegengaan enerzijds, anderzijds ook critical skills bij de mensen verhogen
er zijn wel EU initiatieven, maar zijn beperkt en gelimiteerd tot threats buiten de EU
grote social media platformen dwingen tot maatregelen en rapportering.  maar mensen kunnen dan overstappen naar kleinere platformen.
disinformatie verspreidt altijd sneller dan eventuele rechtzettingen

daarom is strategic foresight nodig (explore plausible future scenarios).  dit soort initiatieven is beperkt bij de EU

media literacy is niet overal in EU goed aanwezig.  zeker de zuidelijke en oostelijke staten scoren slecht.  MIL initatieven moeten zich daarenboven best focussen op groepen die veel getarget worden.
journalisten spelen een bijzondere rol.  zij kunnen (onwetend) disinformatie over migratie verspreiden.  belangrijk aspect hierbij is de toegang tot reliable data over migratie.

Recommendations

- expand monitoring activities through coordinated multistakeholder initaitives
- establish real time and early warning systems
- use foresight to gain a first mover advantage
- increase media literacy skills
- promote migration literacy through subject-specific training for intermediaries (bv journalisten
- apply segmentation and targeting to media literacy efforts
  

de auteur benoemt een aantal eigenschappen van disinformatie en hamert op prebunking.   initiatieven zijn er wel al maar niet genoeg en niet genoeg gecoordineerd.  er wordt vooral gehamerd op menselijke initiatieven, policies, commissies, organisaties, etc en weinig focus op (AI) gebaseerde IT tools

** (R) Examining the Problem of Misinformation among the Indian Diaspora in Australia


in deze paper wordt gefocused op misinformation op indische diaspora in australia.

vele verschillende geloofsgroepen in indie.  ook in de diasporas in australie. meer en meer division en haat tussen de groepen.
Hindutva groep lijkt meer prominent in disinformatie verspreiding (meer similarity met westerse waarden)
australische politici worden voor de kar gespannen van hindutva groep.  politici begrijpen de culturele verschillen niet en laten zich fotograferen op "foute" events / organisaties met bv traditionele kledij die specifieke betekenis heeft.


er staan fake "experts" op in de indian groups die zaken komen vertellen waarvoor ze geen kwalificaties hebben
australische mainstream media geven meer coverage aan pro-hindutva verhalen
specifiek voor covid misinformatie : er is te weinig aandacht voor cultuur specifieke misinformatie (cow urine as a cure)

efforts om mis/disinformatie tegen te gaan bestaan wel in australie.  maar moeten aangehouden worden.  er is nood aan community based fact checking (grote diversiteit in subculturen)
newsroom diversity moet gestimuleerd worden (elke subcultuur moet aanwezig zijn in het media landschap)
journalisten moeten meer aware worden rond disinformatie technieken

** (R) Factuality challenges in the era of large language models and opportunities for fact-checking


bekijken van LLMs in de context van factuality en fact-checking. 
hallucinaties genereren text die niet factually correct is.

aantal risicos verbonden aan LLMS
- antwoorden in formele en zelfzekere toon, ook al is het nonsens
- mensen queryen ze vaak op actuele topics (bv covid) maar LLM heeft onvoldoende kennis
- LLMs worden nu vaak gebruikt als search engine.  maar itt search engines geven LLMs hun bronnen niet weer (transparency)
- LLMs kunnen heel makkelijk ingezet worden door mensen met malicious intent (bv phishing emails, political misinformation, ...) LLMs do not know what they do not know.
- LLMs kunnen biased zijn door datasets


er zijn dus een aantal fact-related risicos verbonden aan LLMs.  dan bestuderen van technologieen en methodologieen om deze te bestrijden.
traditioneel wordt fact-checking gebruikt, maar chatbots / LLMs worden vaak privately gebruikt.  **AI literacy en user awareness is dus zeer belangrijk**.
voornaamste challenges :

- citation gaps : niet transparent zijn van bronnen van LLMS
- grounding deficiency : LLMs zijn niet sterk in het gestructureerd en precies genereren van output bij complexe, detailrijke content in realtime events.
- truthfulness : vaak factueel incorrect.   goed in deduceren, minder in induceren.
- confident tone : "authoritative liar".  zijn niet in staat om aan te geven dat ze niet zeker zijn.
- fluent style : vlotte babbel overtuigt mensen (ook van onzin)
- direct use : mensen gebruiken chatbots privately.  ontsnapt dus aan controle van fact-checkers / ...
- halo effect : mensen nemen aan dat omdat LLMs goed zijn in 1 area, ze ook alles weten over andere areas
- outdated knowledge : knowledge cutoff
- unreliable evaluation : het is moeilijk om factuality van LLMs te meten.  er zijn maar een paar datasets, en die zouden ook kunnen gebruikt zijn om de LLMs in kwestie te trainen


gevaren door malicious use van LLMS

- personalized attacks : het is heel eenvoudig om bv emailhistoriek mee te nemen in een personalized attack
- style impersonation : de stijl van publieke figuren kan nagebootst worden
- bypassing detection : LLMs kunnen eindeloos variaties blijven maken, die niet noodz allemaal gedetecteerd worden.  heeft cumulatief effect
- fake profiles : fake social bots op social media platforms
  

addressing the threats

- alignment and safety : grote AI spelers trachten wel aan alignemnt te doen, maar open source modellen blijven hier achter
- RAG : accurater antwoorden genereren door externe kennis
- hallucination mitigation : modellen zo bouwen dat ze cross checks doen op consistentie van de output
- knowledge updating and maintenance : updated kennis injecteren in LLM (met nodige ripple effects)
- better evaluation : er zijn wel nieuwe score, much room for improvement
- recognizing AI generated content : momenteel staan we hier nog niet ver, in de toekomst verbeteren
- content authenticity and provenance : soort signing van human generated text zodat duidelijk is dat dit human generated is


LLMs inzetten voor fact-checking :
- fact checking support : LLMs inzetten voor summarizing, transcribe texts, generate lists of facts / statements, daarna door mens fact checken.  repeated false claims detecteren.
- stance detection : identifying political beliefs in a text (altijd gevaar van data leakage)
- domain specific verification : search info in a collection of verified documents

  bij elk van deze mogelijke uses van LLMs bij fact-checking moet je rekening houden met de beperkingen van LLMs (bv data leakage, missen van information, ...)!

** (R) Fake news detection: Taxonomy and comparative study

studie van eind 2023, taxonomie van fake news detection.

abstract :
kijken naar 1/ type of features used 2/ fake news detection perspectives 3/ features representation methods 4/ classification approaches.
doen ook empirische test.  transformers are the winners.

introduction :

RQ1: What is the impact of feature extraction methods on model performance?
RQ2: Does the use of transformer models as representation learning have higher performance or as fine-tuning?
RQ3: Would combining feature extraction methods improve performance in the fake news detection task?
RQ4: Which methods are more cost-effective?


laatste vraag kan relevant zijn indien men het en masse zou toepassen bv bij facebook.  misschien te vermelden?
hier wordt gekeken naar transformer models zowel als feature extractors en als classifiers. (betere resultaten wanneer gebruikt als feature extractor dan als classifier)

characteristics and detection perspectives :

content features vs social context features 

content features : textual features, style based features, visual based features
social context features : user information, network information, network propagation,

artikel bevat zeer goed overzicht figuren die kunnen helpen bij het beschrijven van de literature review
mooi overzicht ook van verschillende gebruikte classifiers.

** (R) Fake News Detection Using Deep Learning: A Systematic Literature Review 

zeer recente studie : juni 2024

hier wordt benadrukt dat imbalanced datasets een probleem zijn bij het trainen van (generieke fake news detectie) DL modellen.

SLR = systematic literature review
TL = transfer learning

er wordt expliciet een RQ gesteld naar de imbalance van datasets :
RQ5: Which solutions deal with different levels of imbalanced datasets (if any)?

zoeken van literatuur : google scholar, ACM digital library database, IEEE xplore database, Scopus.

meest onderzocht zijn BiLSTM en CNN modellen.

challenges :

potential for overfitting
use of accuracy on imbalanced datasets
modellen presteren niet altijd goed over datasets heen

transfer learning willen we vaak gebruiken omdat het goedkoper is (bv omwille van veel meer gelabelde data) om het model op een ander domein te trainen en dan te transfereren.

zeer goed overzicht van verschillende manieren van transfer learning, fine-tuning, pre-training etc.

dealing with imbalance :

- over/undersampling
- algorithmic change : op een andere manier leren en classificeren (meer belang hechten aan minority class)
  
imbalance in de context van fake news :
- accuracy zegt weinig, F1, precision en recall zijn ook nodig.
- weinig studies die gekeken hebben naar het effect van imbalance bij fake news detection


open issues :
- lack of labelled data
- potentially biased datasets
- lack of benchmarks
- transfer learning solutions not sufficiently explored
- class imbalance not sufficiently explored
- limited understanding of fake news dynamics
- real world applicability.
  
** (R) Fake news, disinformation and misinformation in social media: a review
studie van eind 2022

RQs : what is fake news in social media, what are existing challenges and issues related to fake news, what are the avabilable techniques used to perform FND

figuur 2 geeft een goed overzicht van de verschillende termen rond fake news

fake news detection : human based, AI based, blockchain based (checking source reliability and establishing the traceability of news content)

** (R) Large Language Models for Propaganda Detection

abstract : 
onderzoek naar gebruik van chatgpt 3 en 4 om propaganda te ontdekken.  testen op dataset met news articles waarin 14 propaganda technieken worden gebruikt met een multi label classification.  variaties door fine tuning en prompting.  vergelijken met een roberta aanpak.  main conclusie : gtp 4 even goed als state of the art

background
propaganda gebruikt loaded language, flag waiving (patriotism, national cause) to manipulate perceptions and shape public attitudes.  fact checking is niet scalable.  er is 1 benchmark dataset voor prop detection, nl semeval-2020 task 11 (detection of propaganda techniques in news articles).  state of the art (SOTA) = roberta.
LLMs voor classificatie typisch via few shot of CoT prompting.  blijven nog wel wat issues over (hallucinatie, repeated output, vanishing gradients, ).  RLHF wordt genoemd.

methodology
bovenvermelde dataset.   5 varianten van chatgpt 3.5 (fine tuned) en 4 (OOBE).  varianten zijn
- gpt 4 base prompt (enkel classif), few shot
- gpt 4 CoT (naast classif ook reasoning weergeven), few shot
- gpt 3 (fine-tuned) base prompt, idem 4
- gpt 3 (fine-tuned) CoT, idem 4
- gpt 3 (fine-tuned) no instruction
alles gedaan met temperatuur 0 (dus minst creatieve setting)

resultaten :
gpt 4 beter dan 3 (10-20%)
gpt3 fine tuned model was overfitted on the training data.  had last van vanishing gradients.
gpt4 niet beter dan baseline voor f1.  uitgespreid over de 14 propa techniques wel betere F1 scores op 7/14 technieken

discussion :

potentials van LLM gebruik :
- goedkoper, eenvoudiger, gewoon via prompting
- RLHF werkt blijkbaar goed (werd geintroduceerd in GPT 4)


challenges :
- gpt 3 te beperkt
- te kleine dataset, daardoor overfitting
- maximum token length = limiting factor
  
** (R) MiDe22: An Annotated Multi-Event Tweet Dataset for Misinformation Detection

creatie van een human annotated dataset met misinformation labels.   ongeveer 5000 engelse en 5000 turkse tweets voor recente events/topics (covid19, ukraine war, immigration, misc)
daarenboven zit in de dataset ook likes, quotes, replies en retweets en extra media (zoals gelinkte video / audio)

de dataset is gebouwd rond 4 topics :
- immigration
- covid
- ukraine
- misc
  
bevat telkens de tweets met 3 labels (false info, true info, other)
bevat ook de user engagements (likes, quotes, replies en retweets) en media elements (audio, video)

testen gedaan op de dataset om baseline scores te krijgen voor model families (BoW, neural, transformer)

bevat een overzicht van andere datasets om misinformation detection op te doen.

tweets werden gecrawled via twitter api academic research access, gebaseerd op keywords.
na crawling zijn deze manueel geannotteerd door mensen (true, false, other).  elke tweet door minstens 2 annotators beoordeeld.  indien disagreement, 3e annotator en dan majority voter.  indien dan nog geen agreement, verwijderen.
temporele analyse gemaakt van de topics in de tweets.  komen meestal snel op en verdwijnen ook weer (relatief) snel

experimenten :

enkel gefocused op detectie van misinformatie obv de text, niet obv de user engagements
8 modellen gemaakt in 3 families : Bag of words (Bow), neural models, transformer based modellen.

- bow : specific terms and phrases (did you know?), linear SVM
- neural : LSTM en bidi-LSTM
- transformer : BERT base uncased en DeBERTa


resultaten :
transformer > SVM > LSTM,bidi-LSTM

** (R) Misinformation, disinformation, and fake news: lessons from an interdisciplinary, systematic literature review

systematic review gedaan van 1261 artikelen (2010 - 2021).  veel research gedaan naar misinformatie maar risico op lack of overview, fragmentation en lack of progress.  hier worden research gaps in de aanpak bestudeerd.

Introduction
zeer veel gelijktijdige research rond misinformatie leidt tot versplintering.  daarenboven lijkt alles dubbel te gebeuren, wordt er weinig over disciplines heen geciteerd.  auteurs stellen dat er te weinig systematic reviews zijn en dat de state of the art in research rond mis/disinformatie ongekend is.  hier 1/ systematic review over disciplines heen en 2/ zien waar er gaps zijn.

er zijn heel wat min of meer gelijke termen, dus het is niet duidelijk wat juist wat is

RQs :
- how has the number of publications evolved over time
- what is the geospatial distrubition of publications
- which discliplines contribute most?
  etc ... 


The article **"Misinformation, disinformation, and fake news: lessons from an interdisciplinary, systematic literature review"** provides an in-depth analysis of these key terms by reviewing scholarly works across various disciplines. Here's a concise summary:

1. **Definitions and Distinctions**:
   - **Misinformation**: False or misleading information shared without intent to harm.
   - **Disinformation**: Deliberate falsehoods shared to deceive and harm.
   - **Fake News**: News content that mimics journalistic practices but is intentionally misleading, often overlapping with disinformation.

2. **Interdisciplinary Approach**:
   - The review integrates findings from communication studies, psychology, political science, information systems, and more, highlighting how different fields conceptualize and study these phenomena.

3. **Drivers and Spread**:
   - Factors contributing to the spread include cognitive biases, emotional appeals, political polarization, and algorithmic amplification on social media.
   - The role of trust, credibility, and social networks in influencing information consumption is emphasized.

4. **Impacts**:
   - Misinformation and disinformation erode trust in institutions, fuel political divisions, and undermine public health efforts (e.g., during the COVID-19 pandemic).

5. **Countermeasures**:
   - Approaches to address the problem include media literacy programs, fact-checking, technological interventions, and regulatory efforts.
   - The effectiveness of these solutions is often debated and context-dependent.

6. **Gaps and Recommendations**:
   - The review highlights the need for standardized definitions and cross-disciplinary collaboration.
   - Future research should focus on cultural and contextual variations, long-term effects, and innovative solutions.

The article underscores the complexity of the misinformation ecosystem and calls for a nuanced, evidence-based approach to combating its harmful effects.

** (R) Misinformation and Its Impact on Contested Policy Issues: The Example of Migration Discourses
abstract :

45000 engelse tweets onderzocht, specifiek in de context van migrants.  misinformatie, concerns, positive and negative attitudes.  misinformatie risicos : negatieve gevoelens tov migranten, bij migranten weinig vertrouwen in overheid, creert een omgeving voor smugglers and trafickers.  solutions : raising awareness, evidence based reasoning

introduction :

studie gaat enkel over misinformation, niet over disinformation.
snelheid van misinfo

misinformation and migration :

misinfo kan geweld en vervolging veroorzaken, enkele voorbeelden : 

- Rohingya crisis in Myanmar
- europese migratie crisis

misschien te bekijken : Greussing, E.; Boomgaarden, H.G. Shifting the refugee narrative? An automated frame analysis of Europe’s 2015 refugee crisis. J. Ethn. Migr. Stud. 2017, 43, 1749–1774.
Mašanović, L.B. The Mythologisation of the Migrant Issue in the Federal Republic of Germany as a Result of the 2015 European Migrant Crisis and Its Effect on Changes in German Migration Policy. Migr. I Etničke Teme 2021, 37, 177–209

- covid crisis (ze verspreiden de ziekte)


methodology

45000 engelse tweets vanaf 20/4/2023.  keyword search op "migrant", "migrants" and "migration".
- dan sentiment analysis (0 - zeer negatief -> 1 zeer positief) and topic analysis gedaan.
- voor topic analyse : 30 meest voorkomende (zinvolle) woorden.
- top 10 most liked and top 10 most retweeted bekeken


results :
topics suggereren negatieve connotatie en ook juridische context.  toch ook (positive) concern towards migrants.
average score 0.6, dus licht positief, range of sentiments in de tweets (30% negatief, 47% positief)
top 10 tweets : vrij negatieve discussies over te veel macht voor europese commissie etc, judges being unable to block migrants, 
top 10 retweeted tweets : negatieve tweets over incidenten, ...

discussion : 

de tweets gaan over de topics die reeds uitvoerig bestudeerd zijn in de context van migratie. TODO welke studie?  goed om te quoten?

corrective measures :
- raising awareness (content-based) : niet proberen de misinformatie te duiden als fout, ook niet probereren migratie als iets positiefs te verkopen, maar algemenere positieve boodschappen verkondigen
- raising awareness (formats) : de counter boodschappen moeten (ook) op social media verspreid worden, films, TV shows, games,
- fact-checking : trusted sources.   kan bv wikipedia zijn.  assess sentiment of text.   TODO kunnen we iets doen met RAG rond wikipedia?  TODO kunnen we iets doen met sentiment analysis?
- code of conducts for media professionals : journalist, ...  goed overzicht van concrete voorbeelden
- education : ...


conclusions :
** (R) On the Risk of Misinformation Pollution with Large Language Models

studie die nagaat hoe je LLMs kan gebruiken om misinformatie te genereren, en wat de impact is op open domain question answering (ODQA) systems.  korte conclusie : LLMs zijn zeer effectieve misinformatie generatoren, performance van ODQA daalt hiermee sterk.  verschil in persuasion van machines en van mensen.   3 mitigation strategieen  : 1/ misinformation detection 2/ vigilant prompting 3/ reader ensemble


3 RQs :
- in hoeverre kan je LLM gebruiken om misinformatie te genereren?
- met welke potentiale harmful gevolgen voor bv ODQA?
- welke mitigation strategies zijn er te bedenken?
  
LLMs kunnen op 2 manieren misinformatie genereren :
- intentional door malicious users
- unintentional door te hallucineren

auteurs zetten een systeem op waarbij die 2 manieren gebruikt worden om misinformatie te genereren en gaan dan na of er impact is op ODQA systemen (die vaak op internet antwoorden zoeken, de assumptie is dan dat de gegenereerde misinformatie door LLMs op het internet terecht komt)
ook RAG systemen hebben een probleem : de extra context die ze genereren kan immers deels door LLM gegeneerde misinformatie zijn.

misinformation generator :
de bedoeling is om misinformation te genereren op basis van een user query.  bv who won the 2020 us election?  4 verschillende prompts om dat te doen :

- GenRead : vragen om gewoon een antwoord te formuleren op de vraag.  misinformatie kan hier dus alleen voorkomen door te hallucineren
- CtrlGen : fabricated fact toevoegen en vragen om de vraag te beantwoorden "in support of the fabricated fact"
- Revise : human written factual article injecteren en dan vragen om een fabricated fact erin te verweven
- Reit : given a question and a predefined response, reiterate the answer 10 times.   bedoeld om downstream NLP systemen te manipuleren


Polluting ODQA with misinformation
kan een QA systeem geforceerd worden om foute antwoorden te geven door veel misinformatie te genereren en te inputten naar het QA systeem?
ODQA systemen :
- retrievers : BM25 en dense passage retrievers (DPR)
- readers : Fusion in Decoder (FiD) en GPT 3.5

dan ODQA systemen laten draaien op dataset met en zonder polluted artikels.  met pollution : zeer kleine hoeveelheden fake genereren / injecteren (0.01%)

resultaten :

- ODQA systemen zijn susceptible voor die attacks. (14-54% performance drop, of zelfs tot 80% afhankelijk van de reader).  toont aan dat ODQA systemen, die getrained zijn op clean data, geen "waarachtigheid" kunnen detecteren
- Reit blijkt in het bijzonder effectief
- LLM generated misinformation is likely om opgehaald te worden door de OD systemen, ook al is er maar een heel kleine proportie misinformatie geinjecteerd
- vragen waarvoor weinig feitelijke ondersteuning bestaat zijn makkelijker te manipuleren met gegenreerede misinformatie


mitigation :
idealiter hebben we misinformation aware QA systemen.  Er zijn drie verschillende manieren van detecteren ontwikkeld :

- Toevoegen van een misinformatie detectiesysteem in het QA systeem, met de bedoeling human generated content te onderscheiden van machine generated content.
- vigilant prompting : Gebruiken van een prompt die expliciet waarschuwt voor misinformatie.  bv toevoegen waarschuwing, pas op, sommige informatie kan bedoeld zijn om jou te misleiden.
- reader ensemble : alle mogelijke contexten groeperen, dan majority voting gebruiken om tot een verdict te komen

Let op, meer context toevoegen leidt niet tot betere resultaten.

Discussie:

Gezien het een kleine moeite is om misinformatie te genereren, is het risico zeer hoog dat het in de toekomst gebruikt wordt.

** (R) Overcoming Racial Harms to Democracy from Artificial Intelligence

zeer recente studie 2024, moet in feite nog gepubliceerd worden

abstract :
wijzen op het gevaar van deepfakes, die veel aandacht krijgen.  AI is threatening democracy.  article beschrijft dat AI moet gereguleerd worden.  naar eigen zeggen : first article to comprehensively identiy the racial harms to democracy posed by AI and offer a way forward.

introduction :
naast deepfakes, ook genAI gevaarlijk.  features zoeken via AI die gebruikt kunnen worden om voting patterns te vinden en te beinvloeden.  bias in datasets.

vaak worden groepen van kleur disproportioneel getroffen door AI tools omdat ze ook minder sociale, politieke en economische invloed hebben.

part 1 :

US heeft stappen gezet naar multiracial democracy (democracy met gelijke rechten voor alle inwoners ongeacht ras) maar is er nog niet.
challenges that remain :

ras heeft grote invloed op voting patterns.   bv trump vnl witte kiezers
white solidarity is aan het groeien
er wordt vaak gerommeld met regels om de voting power van groeiende groepen people of color te beperken.

part 2 AI and democracy :

mogelijke inzetten van AI met impact op democratie :
- deepfakes, genAI
- recommender systems
- image classification systems
- decision systems : can be used to determine whether to purge particular names from voter registration rolls or deem particular absentee ballot signatures as forged
  

vele stakeholders : bedrijven die foundational models ontwikkelen, infrstructuur bedrijven, ontwikkelaars, ... maar ook federal, state committees tot foreign and individual actors die proberen beinvloeden

part 3 AI racials harms to democracy :


uiteraard deepfakes.  teksten genereren en sturen naar stembureaus om te te overstelpen.   data mining om district maps te manipuleren zodat de uitslag wijzigt
automatic content moderation op social media werkt soms minder goed bij people of color.
automated systems die beslissen welke ballots ongeldig zijn kan onpartijdig zijn omwille van dataset biases
genAI is nu zo simpel te gebruiken dat technische kennis niet meer nodig is -> trolls impersonating others, discourage people from voting, make supremacy theories more mainstream, ...

POC = people of color

racial impersonation :
- bv russische group die facebook pagina had opgezet om POC op election day te overtuigen van niet te gaan stemmen.  deden ze door te doen alsof ze zelf POC waren.
- infiltreren in zwarte gemeenschappen
- popular omdat het een effectieve manier om confusion te verspreiden is


daarenboven is het gebruik van (gen)AI goedkoop genoeg dat zelfs een enkeling overtuigende videos bv kan maken
voldoende goede talenkennis van AI modellen volstaat om overtuigend te doen alsof je behoort tot een andere etniciteit.
je kan gemeenschappen ook overspoelen met variaties van dezelfde boodschap
synthetische content kan trust in politics in het algemeen verminderen, en bij POC is de trust al minder.

microtargeting POC
- mogelijk om te microtargeten met genai tot the individual level.  kan ook gebruikt worden voor campaign messaging.  de boodschap kan dan veel overtuigender gemaakt worden.

fueling cultural anxiety :
- ai laat toe om makkelijk content te genereren dat disconent, angst overdrijft.
- gebruik door white supremacists


foundational models zijn getrained op biased datasets en zijn, hence, biased.  AI has trouble recognizing those who are underrepresented in the training data.  daardoor kunnen AI based systemen unfaire beslissingen nemen voor bepaalde bevolkingsgroepen.

AI modellen gaan ook niet op gelijke wijze met iedereen om.   omdat ze vaak mathematisch werken met gemiddeld vlakt dit veel uit naar de "common" factor

grote bias voor engels in LLMs.  daarenboven is blijbkaar veel niet-engelse content machine learning translated van engels (mmm ...)

racially biased content moderation : AI tools die aan content moderation doen kunnen disproportioneel vaak content van POC verwijderen.  ook meer accounts die worden verwijderd.  "talking about racism is racist"

AI in government surveillance : slechtere gezichtsherkenning waardoor foute mensen worden "herkend", automatische matching van betogers met social media profielen, ...
chilling effect = mensen houden zich gedeisd ipv te protesteren uit angst voor government surveillance / herkenning / represailles / ...

part 4 principles to overcome ai threats to democracy :

bestaande wetgeving is onvoldoende.
EU staat verder in wetgeving en verbiedt "cognitive behavioural manipulation of people or specific vulnerable groups".
bijkomende wetgeving nodig.
principles zijn toepasbaar op regulators, ontwikkelaars en social media platforms.

principles :

- Anticipate Racial Harms to Democracy : bij design rekening houden met racial harms
- Facilitate Pluralism and Prevent Algorithmic Discrimination : kijken naar bias in datasets, slechts enkele foundation modellen die opnieuw en opnieuw worden gebruikt (met bias)
- Mitigate Racial Disinformation and Manipulation: de nodige wetgeving op zetten
- Provide Meaningful Accountability : bedrijven / ontwikkelaars van AI modellen should be held accountable bij racial discrimation

** (R) Racial Disinformation, Populism and Associated Stereotypes across Three European Countries during the COVID-19 Pandemic

Studie die nagaat welke onwaarheden verteld worden rond immigratie, voor en na de COVID-19 pandemie en dit in drie verschillende verschillende landen (Frankrijk, Italië en Spanje). 
Onder raciale onwaarheden verstaan we zaken die verteld worden over etnische groepen die zogenaamd de veiligheid of gezondheid van groepen mensen zou beinvloeden.

Emigranten worden vaak gestereotypeerd als volgt:
- gewelddadig, moordenaars, verkrachters,…
- lui, ziek, vies,…

Racial hoaxes zijn gefabriceerde verhalen die er specifiek op gericht zijn om discriminatie en racistische attitudes tegenover migranten te bevestigen

overal in europa meer populisme.   Er zijn verschillen tussen landen.
- FR : rassemblement National komt op.  save french identity.  vrij recent.
- IT : langere traditie, lega nord, forza italia, brothers of italy
- ES : (nu) minder right wing party, alhoewel dit ook hier opkomt


auteurs gebruiken een dataset met 239 racial hoaxes, deels in frans, deels in italiaans en deels in spaans.  het gaat om gedebunkte info bv van social media of news outlets related to immigration.   te bekijken voor thesis??

dataset werd automatisch vertaald na te zijn gepreprocessed (normalisatie, stopword removal, multiword verification, lemmatisation)

The study *"Racial Disinformation, Populism and Associated Stereotypes across Three European Countries during the COVID-19 Pandemic"* explores how disinformation intertwined with populist narratives during the pandemic in **France**, **Italy**, and **Spain**, reinforcing racial and ethnic stereotypes. Here are the key takeaways:

### 1. **Role of Disinformation in Stereotyping**
   - Racial disinformation primarily targeted immigrants, portraying them as threats to public safety, cultural identity, and economic stability.
   - This disinformation was amplified during the COVID-19 pandemic, linking minority groups to public health risks, such as spreading the virus.

### 2. **Populism's Use of Disinformation**
   - Populist political actors in these countries leveraged disinformation to create polarized narratives, often portraying an "us vs. them" dynamic.
   - The rhetoric included framing migrants as invaders, economic burdens, or cultural threats, aligning with the populist strategy of blaming elites and minorities for societal issues.

### 3. **Country-Specific Findings**
   - **France**: Populist movements like the Rassemblement National used disinformation to stress the "threat" posed by immigrants to French national identity, particularly through anti-Muslim narratives.
   - **Italy**: Disinformation capitalized on the migration crisis, with far-right parties, such as Lega Nord, framing immigrants as economic and cultural dangers.
   - **Spain**: Anti-Muslim and anti-immigrant sentiments were amplified, often linked to historical tensions and fears of cultural erosion.

### 4. **Emotional Amplification and Media Dynamics**
   - Disinformation often exploited emotions like fear and anger, making these narratives more shareable and impactful on social media.
   - Fake news aligned with existing stereotypes, reinforcing prejudices and widening societal divisions.

### 5. **Impact on Society**
   - Increased xenophobia and polarization.
   - Strengthened populist and far-right political movements.
   - Erosion of trust in democratic and media institutions.

### 6. **Recommendations**
   - Promote **media literacy** to help the public critically analyze misinformation.
   - Strengthen **regulatory frameworks** for digital platforms to curb disinformation.
   - Develop **inclusive narratives** that challenge stereotypes and foster social cohesion.

### Conclusion
The study highlights how racial disinformation, amplified during crises, reinforces societal divides and bolsters populist agendas. It underscores the need for systemic solutions to combat these harmful dynamics and protect democratic values.

** (R) SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection

gaat specifiek over Out Of Context (OOC) misinformation, waarbij echte fotos met valse text worden gebruikt.  de tool SNIFFER is ontwikkeld, een multimodal LLM dat specifiek getrained is voor OOC detection and exaplanation.  detecteert inconsistenties tussen foto en tekst en gebruikt ook externe bronnen voor context verificatie. state of the art resultaten en accurate en overtuigende explanations

MLLM = multimodal LLM, bv InstructBLIP

intro

huidige aanpak is unified latent space voor foto en tekst.   werkt wel maar exlanations zijn niet overtuigend.
normale MLLMs detecteren verschillen in foto en tekst niet goed, gezien ze hier niet op getrained worden.  daarenboven kunnen MLLMs geen gebruik maken van real time of andere externe informatie.

sniffer tool specifiek voor OOC detectie.  The process involves using GPT-4 to generate sophisticated training data with clear answers and reasoning, and then fine-tuning the InstructBLIP model in two steps to make it better at understanding and responding to instructions.  Gebruikt internal checking (text en foto consistent?) en external checking (reason between text and foto?)


two-stage instruction tuning = a structured approach to training:

    In the first stage, the model is exposed to simpler or foundational tasks.
    In the second stage, the model handles more complex or nuanced instructions, improving its ability to follow and reason through tasks effectively.

related work :

huidige oplossingen gebruiken meestal ofwel internal checking ofwel external checking.  werken wel min of meer maar geven geen explanation, wat nodig is voor  trust bij debunking.

instruction tuning = finetunen van een model om specifieke human generated instructies te kunnen volgen.  wordt getrained door instructies en antwoorden in een dataset te steken.  het model leert hierbij beter hoe mensen typisch instructies doorgeven aan een LLM.  belangrijk voor chatbots bv.  het model generaliseert hierdoor beter bij ongeziene taken.

method :

zeer kort samengevat :
- extraheren van de entiteiten uit de foto
- instructie : hier is de text en hier zijn de visual entitities, zijn deze wrongly used?  (internal checking)
- tegelijkertijd : externe context opvragen van de foto (internet).  hier zoeken we het originele news article waarin de foto voor het eerst gebruikt werd.
- instructie : hier is de text, en hier is externe context, is de text supported door de externe context? (external checking)
- dan nog laatste instructie : combineer de resultaten van internal en external checking en formuleer een finaal antwoord.


experimenten :

bedoeling was zeker ook om explanations te genereren

baseline vergeleken met
- SAFE : input image -> sentence -> sentence vergelijken met caption
- EANN : uses adversarial training
- VisualBERT
- CLIP
- DT-Transformer
- CCN : gebruikt ook retrieved external evidence
- NEU-sym detector
  
nog een aantal ablation studies gedaan om de impact van verschillende componenten van sniffer aan te tonen

resultaten
sniffer outperforms all baselines

explainability resultaten zowel quantitatief als kwalitatief (mensen) beoordeeld.

resultaten van sniffer ook 11% beter dan GPT-4V (multimodel versie van GPT 4, de V staat voor video). kortom, een (relatief) klein task-specific model kan een zeer groot multimodal model outperformen.

cruciaal om te begrijpen is dat OOC detectie iets is wat MLLMs typisch niet doen.  MLLMs moeten tekst genereren uit fotos en trainen dus op de match tussen video en tekst, niet op de mismatch.

** (R) The impact of disinformation campaigns about migrants and minority groups

rapport van de EU rond disinformatie getarget op minority groups.
er is een link tussen disinformatie en fundamentele rechten van minority groups (human dignity, equality, the rule of law and solidarity)

men baseert zich op de EUVsDisinfo database.  hierin zitten relevante articles rond migrants en moslims.
ook te kijken naar kaggle datasets.

klassiek verhaal :
- migranten en moslims zijn een threat voor identity, criminal threat, economic threat

naast euvsdisinfo db, ook bijkomend data verzameld voor roma, migranten en moslims, joden, aziaten, russiche minderheden in eu lidstaten

disinformatie campaigns zorgen ervoor dat er meer haat is tov de getargette groep, minder solidariteit and tolerance

aanbevelingen :
- er is al een strengthening aangekondigd van Code of Practice on Disinformation en Digital Services Act.   daarin zou specifieke bescherming voor minority groups moeten opgenomen worden.   bv verplichte fact-checking
- minorities zouden met een positief tegenverhaal moeten kunnen komen (te ondersteunen door de EU)
- trainen van journalisten, herkennen van disinformatie zodat ze dit niet ongewild verspreiden
- media literacy programs voor algemeen publiek
- member states moeten minority specifieke media literacy programs opzetten
- essential info ook in minority languages verpspreiden (bv covid 19)
- support fact-checking en prebunking
- platformen :
  - verplichten illegale hate speech te rapporteren (naast verwijderen)
  - verplichten accounts te labelen die politieke info verspreiden
  - verplichten identiteit van mensen / orgs die politieke boodschappen na te trekken


conclusies :

shift in disinformatie van fake facts naar emotionele statementes om bestaande sociale problemen te verergeren.  vaak domestically created.
disinformatie is zeker niet alleen van foreign actors, ook vaak domestic

acties nodig op vlak van :
- change the media ecosystem
- increasing societal resistance (fact checking, media literacy, journalisten)
- minorities : positief tegenverhaal, zitje aan de tafel, specifieke media literacy programs
- platforms : zie hierboven, labelen, rapporten, checken identiteit
  

chatgpt :


The EU report on *The Impact of Disinformation Campaigns about Migrants and Minority Groups in the EU* explores how targeted disinformation affects vulnerable groups and undermines democratic processes. Key takeaways include:

1. **Targets of Disinformation**:  
   - Migrants, Roma, Jews, and Muslims are among the most targeted groups. These campaigns often spread false narratives about public health threats, criminality, or cultural incompatibility.  
   - For instance, during the COVID-19 pandemic, disinformation accused migrants of spreading the virus or evading quarantine, fueling xenophobia.

2. **Actors and Motivations**:  
   - Both foreign actors, such as Kremlin-linked groups, and domestic far-right entities play a role in disseminating disinformation. The motivations range from fostering division and eroding trust in democratic institutions to influencing electoral outcomes.

3. **Impacts on Minority Rights**:  
   - Disinformation undermines fundamental rights like equality and human dignity, often inciting discrimination or even restrictive measures against specific groups. For example, the Roma community faced disproportionate restrictions during the pandemic due to disinformation.

4. **Social Cohesion and Democracy**:  
   - Disinformation creates an "out-group," weakening social cohesion and promoting intolerance. It can also erode trust in governance and media, which is critical for democracy.

5. **Policy Recommendations**:  
   - The report emphasizes enhancing media literacy, fostering independent journalism, and improving monitoring and countering of disinformation. Strengthening the resilience of minority communities is also highlighted as essential to combat these campaigns effectively.

Overall, the report underscores the need for EU-wide collaboration to address disinformation and protect vulnerable groups while reinforcing democratic values.

** (R) The perils and promises of fact-checking with large language models


abstract :
evalueren van fact-checking capabilities van LLMs door queries te sturen, contextual data op te halen en decisions te maken.   belangrijk : ook uitleggen wat de reasoning is.
gpt-4 > gpt-3.  maar accuracy varieert afhankelijk van query language.  LLMs tonen dus potentieel, maar research needed om te begrijpen wanneer ze falen en waarom.

introduction :

manual fact checking volstaat niet omwille van volume.
er zijn wel een aantal issues bij het gebruiken van LLMs voor fact-checking.

- data leakage : fact-checks kunnen in de training data zijn opgenomen
- weinig explainability van LLMs (OOBE)


reeds bestaande datasets / tasks : RumourEval, CLEF Checkthat, FEVER, ClaimBuster.
die splitsen meestal het probleem van fact checking op in meerdere componenten : detection, contextualisation, verification

bestaande aanpakken meestal gebaseerd op fine-tuned SLM (Bert / Roberta) zonder context, ook al wat testen met (fine-tuned) gpt-3.5.  dit is allemaal zonder bijkomende context

SOTA haalt nu wel bijkomende context (evidence retrieval) op.  probleem hierbij is dan wel dat er vanuit wordt gegaan dat de opgehaalde evidence (bv via google search) "waar" is.  soms wordt met knowledge databases/graphs gewerkt, maar dit impliceert dat alle entiteiten en relaties in de wereld in de graph zitten (wat niet realistisch is)

hier testen gedaan met LLM (gpt-3.5 en 4) met en zonder externe context.  LLM agent kan ook web searches doen. testen op != talen.  results : gtp-4 > gpt-3.5.  incorporating contextual information greatly improves accuracy.  ivm talen : grote boost als je niet engels eerst naar engels vertaalt en dan laat classificeren


methods :

LLMs gebruiken op 2 manieren :
- basis ondervragen met statement, author, date of statement, geen toegang tot internet en vraag of waar / niet waar
- versie waarbij LLM google kan queryen (maar met filter op domeinen zodat de fact check niet gevonden wordt).  dan de previews van de google search resultaten mee in LLM inputten (niet de hele webpagina aangezien de context window dan geflood wordt).

beide versies van de LLM (?) kan google queryen door het Reasoning and Acting (ReAct) framework te gebruiken.  kan beslissen om google te queryen indien zinvol.  max 3x 10 resultaten van google ophalen en dan een finale beslissing nemen.
claim -> formulate query -> google (evt -> formulate query) -> assessment

testen op gpt4 en 3.5 op 2 datasets, 1 van politifact en 1 van datacommon (multi-lingual)
politifact : datum, auteur, claim, 6-way classificatie (true, mostly true, half true, mostly false, false, pants on fire).  per gpt 2 experimenten, met en zonder context (context betekent dat google queries kunnen gedaan worden)

datacommons dataset : https://datacommons.org/factcheck/
hier wat pre-processing op gedaan naar uniciteit van schaal (4 classes), weggefilterd wat niet kan gemapt wordn etc
hier wordt gekeken naar de performance van gpt3.5 met en zonder context met en zonder vertalen naar engels.

google translate met python : https://pypi.org/project/googletrans/

resultaten :

politifact dataset :

gpt4 > gpt3.  als er los wordt geevalueerd (indien werkelijke label is true, mostly true, half true en het gevonden label is ook 1 van die 3 dan tellen we het goed) dan goede resultaten.   bij exacte matches op labels zeer matig resultaat.  met context is - gemiddeld gezien - beter dan zonder context.  de accuracy over time lijkt ook te stijgen, dwz recentere statements worden beter beoordeeld dan oudere.  in ieder geval beter in false statements te judgen dan true statements.

blijkbaar geen accuracy degradation na de knowledge cutoff date van de gpt modellen (komt dit door RLHF?)

multilingual dataset : 
bijna altijd betere resultaten door eerst te vertalen naar engels.

conclusies :
gpt 4 > gpt 3.5
auteurs claimen goede performantie alhoewel dat toch van de manieren van meten afhangt
naar engels vertalen beter dan oorspronkelijke taal.  wel verschillen per taal merkbaar
gezien geen degradatie in fact-checks na knowledge cutoff besluiten de auteurs dat er niet noodzakelijk data leakage (fact-checking kennis) voorvalt als je externe context injecteert

fact-checking tools moeten gezien worden als hulpmiddel voor human fact-checkers om hun taken te versnellen

** (R) The rapid diffusion of fake news: An analysis of content on migration, refugees, and conflict on international fact-checking platforms
abstract :

RQ1 : welke media platforms worden het meest gebruikt om refugee-focused fake news te verspreiden?
RQ2 : welke content types worden meestal gebruikt hiervoor?
RQ3 : welke zijn de common topics?

zeer kort : vooral facebook en X.  vooral titles en text-supported videos.  topics : protesten in FR over doogeschoten tiener, fake news rond conflicten, en rond financiele steun, kansen en accomodation voor refugees.

introduction :
conventionele media hebben wetten, ethische codes, etc die niet gelden voor social media

IFCN = assocatie van meerdere fact-checking organisaties.  opgericht om soort van keurmerk te zijn voor factchecking organisaties.  hebben richtlijnen rond hoe factchecking te doen.
https://www.poynter.org/ifcn/

mensen sluiten zich op in hun bubbel (echo chambers) waar iedereen gelijkgezind is en alles voor waar wordt aangenomen en wordt doorgestuurd.  fake news circuleert veel sneller dan echt nieuws.
hier kijken naar disinformatie, kanalen en topology over verschillende landen gedurende eenzelfde periode.

methodology & results :

126 fake news articles uit conventional press en social media.  top 10 landen met meeste refugees + FR, UK en ierland, en RO mee opgenomen
blijkt dat de landen die het meest refugees ontvangen slechts heel weinig fake news articles hebben rond refugees (en idem voor omgekeerd - ierland)

vnl social media verspreiden fake news.  buiten de UK, waar de conventional press de meeste fake news articles verspreidt.

content typology
er bestaat een framework voor
textual properties : title, hyperlink, resource en content
multimedia properties : image, video, audio, gifs/animations

users tend to believe and share video content more

ongeveer de helft van fake news is video, vnl op facebook, x en tiktok

common topics :
protesten in FR over doogeschoten tiener, fake news rond conflicten, en rond financiele steun, kansen en accomodation voor refugees

** (R) Ukrainian Resistance to Russian Disinformation: Lessons for Future Conflict

rapport geschreven door een defence organisatie van USA.  hoe tracht ukraine de misinformatie stroom door rusland tegen te gaan?
dateert van april 2024.

lessons learned over de aanpak van disinformatie van rusland door ukraine.  evt voor latere militaire conflicten in usa.

ondersteunen van influencers kan belangrijk zijn om international support te verwerven.

disinformation tools :

- debunking (fact checking)
- prebunking
- promulgation of proactive information

3 soorten malinformation :

- disinformation : intentionally false or fabricated information
- misinformation : false information that is unintentionally shared
- malinformation : true information that is shared out of context with the intent to hurt a person / organisation

3 theaters :

- ukraine
- russische staat
- internationele community


theater ukraine : 

een aantal "shaping operations" worden besproken, dwz initiatieven die reeds voor de oorlog zijn gestart om disinformatie tegen te gaan.  meerdere organisaties zijn opgericht tussen de invasie van de krim en de huidige oorlog, bv Center for Strategic Communication (CSC)
media literacy education is belangrijke tool.
fake aanval verzinnen om een justification te hebben om een oorlog te starten = false flag operation.  prebunking is false flag operations onderscheppen, en erover communiceren aan een breed publiek om ze zo te voorkomen (publiek weet dus wat voor disinformatoin er zal uitgestuurd worden).  


telegram is een russisch social media platform.   veel gebruikt in ukrain. geblokkeerd om stroom aan disinformatie af te snijden.
meta (facebook) heeft een third-party fact checking initiatief, waarbij ukrain als third party fact checker optreedt.   de third party geeft labels aan false content en meta stuurt deze labels mee, en limiteert de verspreiding van zulke stories.
Detector Media is een organisatie binnen ukrain die AI tools ontwikkelt om russische disinformatie te ontdekken en tegen te gaan

media literacy is over het algemeen goed.  maar in het zuiden en oosten, waar de disinformation geconcentreerd is, is dit wel minder

disinformatie heeft ook een cumulatief effect, dwz het blijft herhaald worden en krijgt zo dan toch voet aan de grond

all in all : efforts van ukraine om disinformatie van rusland tegen te gaan zijn redelijk succesvol

theater rusland

veel propaganda en fake kanalen vanuit rusland.
ukraine probeert om manueel reacties te schrijven op social media
weinig succes want russische bevolking blijft achter regering en oorlog staan

theater international

rusland lanceert heel wat media campagnes om politiek gevoelige discussies te starten om ukraine in diskrediet te brengen, voornamelijk bij de US.   zodat zij de funding zouden stoppen.

ook in het zuiden (afrika, ...) worden campagnes gedaan.  bv ook met bezoeken (anti imperialistisch overkomen)

vooral in het begin van de oorlog veel attention gegenereerd in het buitenland.  nadien zakt dit wat weg


lessons

het gaat om lessen specifiek voor US.   basically wordt er gezegd, zorg voor 3 theaters, zorg voor voldoende organisaties, trust, media literacy van de mensen (ook van soldaten), ...


debunking : correcties publiceren over foute informatie die is gepubliceerd.
prebunking : beschermen van audiences tegen foute informatie door op voorhand al over die foute informatie te communiceren en mee te geven dat deze fout is.
proactive narrative campaigns : op voorhand al de "juiste" informatie doorsturen

** (R) Understanding anti-immigration sentiment spreading on Twitter

analyse op (anti)immigratie tweets.  anti-immigratie community is kleiner maar meer actief.  verspreidt ook sneller zijn tweets.

doel :
- Determine the extent of polarization of social media immigration sentiment;
- Identify the key producers and spreaders of social media immigration-related content;
- Measure the speed at which this content is disseminated through social media immigration communities.  de snelheid waarmee informatie doorstroomt is van belang, want hatefull speech gaat sneller.  conspiracy trager < science based < hate 

een zeer kleine minderheid van de gebruikers (0.1%) kan een zeer grote meerderheid van fake news doorsturen (80%)



** (R) GRAG: Graph Retrieval-Augmented Generation

abstract :
ipv te focussen op single comments, bijkomend ook citation/knowledge graphs injectern in LLMs.   Doel : betere generatie.  experimenten op "graph reasoning benchmarks" (waarbij multi-hop reasoning nodig is) tonen veel betere resultaten dan gewone RAG setups.

Introduction : 
real world teksten zijn meestal niet geisoleerd maar gegroepeerd in textual graphs (bv entity relations in knowledge graphs).  die netwerk info is cruciaal voor correcte retrieval en voor betere generatie.
vragen : hoe efficient retrieven? hoe informatie van de graph in de LLM krijgen?
voor efficiente retrieval : we willen geen NP hard probleem van exhaustively alle subgraphs checken uitvoeren
voor generation : speciale prompt die de hierarchie en de tekst capteert in tekstuele vorm
frozen LLM with GRAG outperforms fine tuned LLM in all cases.
er worden "graph multi-hop reasoning benchmarks" gebruikt.

Problem Formalization :
graph wordt gerepresenteerd als nodes, edges en verzamelingen van teksten gekoppeld aan nodes en edges.   dus een 4-tupel.
kans op output gegeven een query en een graph is gegeven door productie van de kans van token y_i gegeven vorige tokens en de concatenatie van de query en de gekozen (zo optimaal mogelijke) subgraph.

Methodology :
eerst subgraph ophalen door slimme combo van zoeken, mergen en prunen.
dan 2 views opbouwen op die subgraph :
- graph view of textual graphs
- text view of textual graphs

probleem wordt benaderend opgelost door ipv subgraphs te zoeken, belangrijke nodes te zoeken.  van die subgraphs wordt een embedding gemaakt, en de embedde query kan dan via cosine similarity zoeken naar semantisch gelijkaardige subgraphs.
daarna nog wat soft pruning (deze component is learned obv afstanden tussen nodes en edges)
daarna alles mergen in 1 subgraph.

views : 
text view of textual graphs: beetje hierarchisch gerangschikte opsomming van de text van de graphs

voor generation : de embedding space tussen graph embedding en text embedding is niet gelijk.   dus wordt nog omgezet met een extra component.

Experiments :
aantal benchmark datasets gebruikt.  Gebruikte metrieken : F1, Hit@1, en recall voor WebQSP dataset.   voor ExplaGraphs wordt accuracy gebruikt.
enkele conclusies :

- GRAG surpasses baseline LLM and RAG
- soft pruning boosts accuracy (remove redundant information)
- GRAG demonstrates great transferability (across graph dataset kan je opgebouwde kennis overdragen)
- Larger LLMs don’t necessarily outperform smaller ones in graph-related tasks without retrieval


** (R) Evaluating Retrieval Quality in Retrieval-Augmented Generation
abstract :
hoe evaluatie doen van het retrieval model van een RAG systeem?  e2e evaluation is computationally expensive.  men introduceert "eRAG", waar elk individueel element in de retrieval list gebruikt wordt door het LLM systeem.  dan de downstream taken evalueren.  de redenering is : als de downstream taak beter is, dan is het retrieved document ook beter geweest.



introduction :
e2e evaluation heeft beperkingen :
- je weet niet welk document heeft bijgedragen tot de output
- resource intensive
- optimizen van zo'n setup heeft vaak behoefte aan user feedback.  dat liefst per document maar met gewone RAG is dat per lijst van opgehaalde documenten.

doel van de paper is de performance van de retriever component te meten door elk document individueel een downstream task te laten uitvoeren en dan te meten wat de perf is van de downstream task.  de downstream task kan bv met accuracy, exact match of rouge gemeten worden.

evaluating retrievers in RAG :

2 aanpakken zijn typisch :
- human judgment
- utilize the downstream ground truth (a retrieved document containing the ground truth is considered relevant)


met eRAG per retrieved document meten wat de perf van de downstream task is.  dit is ook goedkoper / efficienter omdat de kost van een transformer kwadratisch stijgt met de input length
retrieval evaluation metrics : score bepalen voor elk document in de retrieved list.  kan precision, recall, mean average precision, ... zijn

experiments :

As the retrieval corpus, we employ the Wikipedia dump of the KILT benchmark and adhere to the preprocessing outlined by Karpukhin et al. [14]  TODO te bekijken!! Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for OpenDomain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 6769–6781. https://doi.org/10.18653/v1/2020.emnlp-main.550


ook te bekijken, retrievers
BM25 implemented in Pyserini
fast vector retrieval with Contriever (facebook) , we use FAISS flat index.


** (R) Evaluation of Retrieval-Augmented Generation: A survey
abstract :

onderzoeken en vergelijken van meetbare metrics van de retriever en generation components.

introduction :

RAG significantly reduces the incidence of hallucinations or factually incorrect outputs.
retriever component :

- sparse retrieval met inverted indexes
- dense retrieval met dense vector encoding


challenges in evaluating RAG systems

je moet het stuk retrieval, generation en het gehele systeem evalueren.

1. retrieval :

hoe meet je precision, recall en relevance?  rekening houden met het temporal aspect (waarheid verandert in de tijd)

2. generation :

hoe faithfulness en accuracy meten van de output?  moet ook relevant zijn tov de query.


3. gehele systeem :

retrieval en generation alleen meten is niet genoeg.   gehele systeem meten, en vooral meten wat de toegevoegde waarde is van de retriever op de generation (vandaar de baseline die belangrijk is)
response latency is een concern.


Unified evaluation process :

er wordt een systeem opgezet figuur 2.  resultaten van retriever en generator enerzijds en ground truth retriever en generator anderzijds.

voor retriever krijg je dan relevance (relevant documents <> query) en accuracy (relevant documents <> documents candidate)
voor generator krijg je dan relevance (response <> query), faithfulness (response <> relevant documents), correctness (response <> sample response)

er zijn ook additional requirements, zoals latency, diversity, noise robustness, negative rejection, counterfactual robustness 

goed overzicht van retrieval metrics. accuracy, precision, recall@k maar ook metrics die de order van de retrieved documenten meenemen zoals mean reciprocal rank, mean average precision,

generation metrics : dan spreken we vooral over coherence, relevancy, fluency, en alignment met human values (BLUE, ROUGE, LLM as a judge, Bertscore,  F1 score) .  echter ook bijkomende metrieken zoals Misleading rate, mistake reappearance rate en error detection rate.




* begrippen

role prompting : Role Prompting assigns a persona to an LLM, such as "teacher" or "salesperson," to guide the style, tone, and focus of responses. Enhances text clarity and accuracy by aligning the response with the role, improving task performance in reasoning and explanation

temperature : hogere temperatuur = output more random.  lagere temperatuur = output met words met de highest probability of occurrence.

a "soft prompt" is an implicit or less structured instruction given to the model. Instead of explicitly stating a task or question, the context or subtle hints guide the model to produce a specific type of response.  Example of a Hard Prompt:   "Write a poem about the ocean in a Shakespearean style." Example of a Soft Prompt: "The ocean whispers tales of bygone eras, as the waves lap gently against the shore..." (Here, the initial text sets the tone and context for the response.)

Hit@1 measures the proportion of times the top-ranked prediction (the first result) is correct. It assesses how often the model's most confident prediction matches the ground truth.

* links

[[https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/][BM25 search algorithm]]
https://github.com/Florents-Tselai/WarcDB
https://github.com/iipc/jwarc
https://medium.com/@samuel.schaffhauser/using-the-common-crawl-as-a-data-source-693a41b3baa9
index lijst downloaden op https://medium.com/@samuel.schaffhauser/using-the-common-crawl-as-a-data-source-693a41b3baa9
https://pullpush.io
https://developers.tiktok.com/products/research-api/
https://sf16-va.tiktokcdn.com/obj/eden-va2/lapz_k4_rvarpa/ljhwZthlaukjlkulzlp/form/research-endorsement-letter.pdf
https://vast.ai/
https://www.shepbryan.com/blog/what-is-gguf

let op : er is EU disinfo lab en eu vs disinfo

https://www.disinfo.eu/publications/disinformation-landscape-in-the-netherlands/
https://www.disinfo.eu/publications/disinformation-landscape-in-belgium/

project dat in belgie socials checkte op disinformatie
https://crossover.social/

mee te nemen?
https://www.reddit.com/r/Antwerpen/


https://euvsdisinfo.eu/ukraine/

commonly used narratives :
https://edmo.eu/publications/disinformers-use-similar-arguments-and-techniques-to-steer-hate-against-migrants-from-ukraine-or-the-global-south-2/

* datasets

[[https://www.kaggle.com/code/akshayr009/fakenewsdetection]]
https://www.kaggle.com/datasets/corrieaar/disinformation-articles
https://www.kaggle.com/datasets/imuhammad/euvsdisinfo-disinformation-database
